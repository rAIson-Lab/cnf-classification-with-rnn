{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import Embedding, RNN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "#Below imports are via https://www.datacamp.com/tutorial/parameter-optimization-machine-learning-models\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set any global values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x205b77d9c90>"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('training_data.pickle','rb') as f:\n",
    "    training_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_to_ix = {\"Found\":0, \"Unfound\":1}\n",
    "all_letters = \"()ab~&|>\"\n",
    "\n",
    "vocab = ['<pad>'] + sorted(set([char for seq in all_letters for char in seq]))\n",
    "\n",
    "\n",
    "n_classes = len(class_to_ix)\n",
    "n_letters = len(all_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "embed_dim = len(vocab)\n",
    "embed = Embedding(len(vocab), embed_dim) # embedding_dim = len(vocab)\n",
    "for element in training_data:\n",
    "    input = [vocab.index(token) for token in element[0]]\n",
    "    input_tensor = torch.tensor(input, dtype=torch.int)\n",
    "    classification = class_to_ix[element[1]]\n",
    "    X.append(input_tensor)\n",
    "    output_tensor = torch.tensor(classification, dtype=torch.int)\n",
    "    Y.append(output_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TTPStyleDataset(Dataset):\n",
    "    def __init__(self, X,Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        sample = [self.X[idx], self.Y[idx]]\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "batch_size = 16\n",
    "input_dataset = TTPStyleDataset(X_train,y_train)\n",
    "\n",
    "test_dataset = TTPStyleDataset(X_test,y_test)\n",
    "\n",
    "def my_collate_fn(data):\n",
    "    (xx,yy) = zip(*data)\n",
    "    x_lens = [len(x) for x in xx]\n",
    "    y_lens = [1 for y in yy]\n",
    "\n",
    "    xx_pad = pad_sequence(xx, batch_first=False, padding_value=0)\n",
    "    yy_pad = torch.tensor(yy)\n",
    "\n",
    "    return xx_pad, yy_pad, x_lens, y_lens\n",
    "    \n",
    "\n",
    "#dataloader = DataLoader(input_dataset, shuffle=True, batch_size=batch_size, num_workers=0, collate_fn=my_collate_fn)\n",
    "#test_dataloader = DataLoader(test_dataset, shuffle=True, batch_size=batch_size, num_workers=0, collate_fn=my_collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxRNN(nn.RNN):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, nonlinearity, bias, batch_first, dropout, bidirectional, output_size):\n",
    "        super(SoftmaxRNN, self).__init__(input_size, hidden_size, num_layers, nonlinearity, bias, batch_first, dropout, bidirectional)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    \n",
    "    def forward(self, x, h_0=None):\n",
    "        output, h = super(SoftmaxRNN, self).forward(x, h_0)\n",
    "        output, seq_len = pad_packed_sequence(output, batch_first=False)\n",
    "        output = self.fc(h[-1])\n",
    "        return output, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SoftmaxRNN(\n",
       "  9, 128\n",
       "  (fc): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#input_size - The number of expected features in the input x\n",
    "rnn = SoftmaxRNN(input_size=embed_dim, hidden_size=128, num_layers=1, nonlinearity='relu', bias=True, batch_first=False, dropout=0.0, bidirectional=False, output_size=2)\n",
    "rnn.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU\n",
    "\n",
    "Referencing \"Dive into Deep Learning\" by Aston Zhang, et al. pages 376-382.\n",
    "Referencing https://medium.com/@anishnama20/understanding-gated-recurrent-unit-gru-in-deep-learning-2e54923f3e2\n",
    "\n",
    "__Below are some notes.__\n",
    "\n",
    "GRU contains the following.\n",
    "* Two gates in which are the **Reset** and **Update** gates. \n",
    "* Candidate Hidden State - Combines information from the input and the previous hidden state that is used to update the hidden state for the next time step. The Long & Short Term memory are now in this candidate hidden state(?)\n",
    "\n",
    "Reset Gate - Determines how much of the previous hidden state to forget. \n",
    "* Takes hidden state h_t-1 and current word x_t apply\n",
    "Update Gate - Determines how much of the candidate hidden state to incorprate into the new hidden state.\n",
    "\n",
    "A bit of recap on Gaussian Distribution: https://stackoverflow.com/questions/12616406/anyone-can-tell-me-why-we-always-use-the-gaussian-distribution-in-machine-learni\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://d2l.ai/chapter_recurrent-modern/gru.html#implementation-from-scratch\n",
    "class GRU(nn.GRU):\n",
    "    #Input size - input_size - Defines the number of features that define each element (time-stamp) of the input sequence.\n",
    "        #This passes the features into the hidden layers that will perform computations. \n",
    "    #hidden_size - Defines the size (amount of features) of the hidden state. Therefore, if hidden_size is set as 4, then the hidden state at each time step is a vector of length 4\n",
    "    def __init__(self, input_size, hidden_size, num_layers, bias, batch_first, dropout, bidirectional, output_size): #Why is it if I have num_layers = 2, we get an error? \n",
    "        super(GRU, self).__init__(input_size, hidden_size, num_layers, bias, batch_first, dropout, bidirectional) #Initializing parent class of our GRU class (in this case it is torch.nn.GRU)\n",
    "        self.gru = nn.GRU(input_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, input, h_0 = None):\n",
    "        output, h = super(GRU, self).forward(input, h_0) #Returns packed sequence only\n",
    "        output, seq_len = pad_packed_sequence(output, batch_first=False)\n",
    "        output = self.fc(h[-1])\n",
    "        return output, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GRU(\n",
       "  9, 128, num_layers=3\n",
       "  (gru): GRU(9, 128)\n",
       "  (fc): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = embed_dim\n",
    "hidden_size = 128\n",
    "num_layers = 3\n",
    "bias = True\n",
    "batch_first = False\n",
    "dropout = 0.0\n",
    "bidirectional = False\n",
    "output_size = 2\n",
    "\n",
    "rnn = GRU(input_size, hidden_size, num_layers, bias, batch_first, dropout, bidirectional, output_size)\n",
    "rnn.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output best to worst trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_params = []\n",
    "def add_to_list_of_params(entry):\n",
    "    list_of_params.append(entry)\n",
    "    list_of_params.sort(key=lambda x: x[\"accuracy\"], reverse=True)\n",
    "    \n",
    "def print_all_trials():\n",
    "    print(\"=========================\\nTrials from Best to Worst\\n=========================\\n\")\n",
    "    for i, entry in enumerate(list_of_params, 1):\n",
    "        print(f\"{i}: \"\n",
    "              f\"Trial #{entry['trial']}, \"\n",
    "              f\"Accuracy = {entry['accuracy']}, \"\n",
    "              f\"Avg Test Loss = {entry['avg_test_loss']}, \"\n",
    "              f\"Hidden Size = {entry['hidden_size']}, \"\n",
    "              f\"Num Layers = {entry['num_layers']}, \"\n",
    "              f\"Bias = {entry['bias']}, \"\n",
    "              f\"Batch First = {entry['batch_first']}, \"\n",
    "              f\"Dropout = {entry['dropout']}, \"\n",
    "              f\"Bidirectional = {entry['bidirectional']}, \"\n",
    "              f\"Num Epochs = {entry['num_epochs']}, \"\n",
    "              f\"Learning Rate = {entry['learning_rate']}, \"\n",
    "              f\"Batch Size = {entry['batch_size']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================================== \n",
      "Trial 1 \n",
      "\n",
      "hidden_size:  64 \n",
      "num_layers:  2 \n",
      "bias =  False \n",
      "batch_first =  True \n",
      "dropout =  0.0 \n",
      "bidirectional =  True \n",
      "num_epochs =  13 \n",
      "learning_rate =  0.01 \n",
      "batch_size =  16 \n",
      "=========================\n",
      "epoch: 0\n",
      "train loss: 0.691025  [   16/141120]\n",
      "train loss: 0.527832  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 0.595550 \n",
      "\n",
      "epoch: 1\n",
      "train loss: 0.633524  [   16/141120]\n",
      "train loss: 0.627528  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.594137 \n",
      "\n",
      "epoch: 2\n",
      "train loss: 0.701585  [   16/141120]\n",
      "train loss: 0.594272  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.593338 \n",
      "\n",
      "epoch: 3\n",
      "train loss: 0.853814  [   16/141120]\n",
      "train loss: 0.484992  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.591596 \n",
      "\n",
      "epoch: 4\n",
      "train loss: 0.477034  [   16/141120]\n",
      "train loss: 0.592743  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.590618 \n",
      "\n",
      "epoch: 5\n",
      "train loss: 0.833911  [   16/141120]\n",
      "train loss: 0.602738  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.4%, Avg loss: 0.582781 \n",
      "\n",
      "epoch: 6\n",
      "train loss: 0.642014  [   16/141120]\n",
      "train loss: 0.568754  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 71.6%, Avg loss: 0.559445 \n",
      "\n",
      "epoch: 7\n",
      "train loss: 0.485964  [   16/141120]\n",
      "train loss: 0.554735  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 73.9%, Avg loss: 0.484085 \n",
      "\n",
      "epoch: 8\n",
      "train loss: 0.506309  [   16/141120]\n",
      "train loss: 0.408338  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.414354 \n",
      "\n",
      "epoch: 9\n",
      "train loss: 0.481858  [   16/141120]\n",
      "train loss: 0.404224  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 77.0%, Avg loss: 0.391165 \n",
      "\n",
      "epoch: 10\n",
      "train loss: 0.189783  [   16/141120]\n",
      "train loss: 0.301892  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 78.0%, Avg loss: 0.369352 \n",
      "\n",
      "epoch: 11\n",
      "train loss: 0.300729  [   16/141120]\n",
      "train loss: 0.311832  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 79.6%, Avg loss: 0.354239 \n",
      "\n",
      "epoch: 12\n",
      "train loss: 0.250965  [   16/141120]\n",
      "train loss: 0.389145  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 81.7%, Avg loss: 0.334332 \n",
      "\n",
      "=========================\n",
      "Trials from Best to Worst\n",
      "=========================\n",
      "\n",
      "1: Trial #1, Accuracy = 81.7063492063492, Avg Test Loss = 0.33433154225349426, Hidden Size = 64, Num Layers = 2, Bias = False, Batch First = True, Dropout = 0.0, Bidirectional = True, Num Epochs = 13, Learning Rate = 0.01, Batch Size = 16\n",
      "============================================================================================== \n",
      "Trial 2 \n",
      "\n",
      "hidden_size:  64 \n",
      "num_layers:  4 \n",
      "bias =  False \n",
      "batch_first =  True \n",
      "dropout =  0.0 \n",
      "bidirectional =  True \n",
      "num_epochs =  10 \n",
      "learning_rate =  0.001 \n",
      "batch_size =  16 \n",
      "=========================\n",
      "epoch: 0\n",
      "train loss: 0.675804  [   16/141120]\n",
      "train loss: 0.520000  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.603865 \n",
      "\n",
      "epoch: 1\n",
      "train loss: 0.515222  [   16/141120]\n",
      "train loss: 0.557369  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.601633 \n",
      "\n",
      "epoch: 2\n",
      "train loss: 0.616275  [   16/141120]\n",
      "train loss: 0.504226  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.599075 \n",
      "\n",
      "epoch: 3\n",
      "train loss: 0.449418  [   16/141120]\n",
      "train loss: 0.674003  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.596624 \n",
      "\n",
      "epoch: 4\n",
      "train loss: 0.660873  [   16/141120]\n",
      "train loss: 0.505973  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 0.594916 \n",
      "\n",
      "epoch: 5\n",
      "train loss: 0.463565  [   16/141120]\n",
      "train loss: 0.526010  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.594481 \n",
      "\n",
      "epoch: 6\n",
      "train loss: 0.472394  [   16/141120]\n",
      "train loss: 0.504796  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.594331 \n",
      "\n",
      "epoch: 7\n",
      "train loss: 0.695286  [   16/141120]\n",
      "train loss: 0.553713  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.594000 \n",
      "\n",
      "epoch: 8\n",
      "train loss: 0.683876  [   16/141120]\n",
      "train loss: 0.628317  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.593945 \n",
      "\n",
      "epoch: 9\n",
      "train loss: 0.566630  [   16/141120]\n",
      "train loss: 0.672385  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 0.593679 \n",
      "\n",
      "=========================\n",
      "Trials from Best to Worst\n",
      "=========================\n",
      "\n",
      "1: Trial #1, Accuracy = 81.7063492063492, Avg Test Loss = 0.33433154225349426, Hidden Size = 64, Num Layers = 2, Bias = False, Batch First = True, Dropout = 0.0, Bidirectional = True, Num Epochs = 13, Learning Rate = 0.01, Batch Size = 16\n",
      "2: Trial #2, Accuracy = 70.94954648526077, Avg Test Loss = 0.5936791300773621, Hidden Size = 64, Num Layers = 4, Bias = False, Batch First = True, Dropout = 0.0, Bidirectional = True, Num Epochs = 10, Learning Rate = 0.001, Batch Size = 16\n",
      "============================================================================================== \n",
      "Trial 3 \n",
      "\n",
      "hidden_size:  32 \n",
      "num_layers:  3 \n",
      "bias =  False \n",
      "batch_first =  False \n",
      "dropout =  0.0 \n",
      "bidirectional =  False \n",
      "num_epochs =  11 \n",
      "learning_rate =  0.001 \n",
      "batch_size =  16 \n",
      "=========================\n",
      "epoch: 0\n",
      "train loss: 0.671329  [   16/141120]\n",
      "train loss: 0.614996  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.603632 \n",
      "\n",
      "epoch: 1\n",
      "train loss: 0.675068  [   16/141120]\n",
      "train loss: 0.670124  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.602754 \n",
      "\n",
      "epoch: 2\n",
      "train loss: 0.774489  [   16/141120]\n",
      "train loss: 0.713557  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.601502 \n",
      "\n",
      "epoch: 3\n",
      "train loss: 0.686489  [   16/141120]\n",
      "train loss: 0.522231  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.600172 \n",
      "\n",
      "epoch: 4\n",
      "train loss: 0.601691  [   16/141120]\n",
      "train loss: 0.527154  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.598429 \n",
      "\n",
      "epoch: 5\n",
      "train loss: 0.592496  [   16/141120]\n",
      "train loss: 0.599054  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.596666 \n",
      "\n",
      "epoch: 6\n",
      "train loss: 0.498458  [   16/141120]\n",
      "train loss: 0.870055  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.595432 \n",
      "\n",
      "epoch: 7\n",
      "train loss: 0.596646  [   16/141120]\n",
      "train loss: 0.709029  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.594354 \n",
      "\n",
      "epoch: 8\n",
      "train loss: 0.755577  [   16/141120]\n",
      "train loss: 0.533053  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.593806 \n",
      "\n",
      "epoch: 9\n",
      "train loss: 0.602030  [   16/141120]\n",
      "train loss: 0.595483  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.593449 \n",
      "\n",
      "epoch: 10\n",
      "train loss: 0.491975  [   16/141120]\n",
      "train loss: 0.608794  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.593031 \n",
      "\n",
      "=========================\n",
      "Trials from Best to Worst\n",
      "=========================\n",
      "\n",
      "1: Trial #1, Accuracy = 81.7063492063492, Avg Test Loss = 0.33433154225349426, Hidden Size = 64, Num Layers = 2, Bias = False, Batch First = True, Dropout = 0.0, Bidirectional = True, Num Epochs = 13, Learning Rate = 0.01, Batch Size = 16\n",
      "2: Trial #2, Accuracy = 70.94954648526077, Avg Test Loss = 0.5936791300773621, Hidden Size = 64, Num Layers = 4, Bias = False, Batch First = True, Dropout = 0.0, Bidirectional = True, Num Epochs = 10, Learning Rate = 0.001, Batch Size = 16\n",
      "3: Trial #3, Accuracy = 70.65192743764173, Avg Test Loss = 0.5930311679840088, Hidden Size = 32, Num Layers = 3, Bias = False, Batch First = False, Dropout = 0.0, Bidirectional = False, Num Epochs = 11, Learning Rate = 0.001, Batch Size = 16\n",
      "============================================================================================== \n",
      "Trial 4 \n",
      "\n",
      "hidden_size:  32 \n",
      "num_layers:  4 \n",
      "bias =  False \n",
      "batch_first =  False \n",
      "dropout =  0.0 \n",
      "bidirectional =  True \n",
      "num_epochs =  15 \n",
      "learning_rate =  0.01 \n",
      "batch_size =  16 \n",
      "=========================\n",
      "epoch: 0\n",
      "train loss: 0.699669  [   16/141120]\n",
      "train loss: 0.665083  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.594482 \n",
      "\n",
      "epoch: 1\n",
      "train loss: 0.447098  [   16/141120]\n",
      "train loss: 0.605600  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 0.593956 \n",
      "\n",
      "epoch: 2\n",
      "train loss: 0.715318  [   16/141120]\n",
      "train loss: 0.611480  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.591867 \n",
      "\n",
      "epoch: 3\n",
      "train loss: 0.610602  [   16/141120]\n",
      "train loss: 0.647161  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.591742 \n",
      "\n",
      "epoch: 4\n",
      "train loss: 0.343112  [   16/141120]\n",
      "train loss: 0.568425  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.587848 \n",
      "\n",
      "epoch: 5\n",
      "train loss: 0.586867  [   16/141120]\n",
      "train loss: 0.614893  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.574466 \n",
      "\n",
      "epoch: 6\n",
      "train loss: 0.595971  [   16/141120]\n",
      "train loss: 0.531928  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 72.1%, Avg loss: 0.556148 \n",
      "\n",
      "epoch: 7\n",
      "train loss: 0.597842  [   16/141120]\n",
      "train loss: 0.425129  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.462802 \n",
      "\n",
      "epoch: 8\n",
      "train loss: 0.394684  [   16/141120]\n",
      "train loss: 0.517291  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 76.5%, Avg loss: 0.381598 \n",
      "\n",
      "epoch: 9\n",
      "train loss: 0.426489  [   16/141120]\n",
      "train loss: 0.311383  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 76.7%, Avg loss: 0.369425 \n",
      "\n",
      "epoch: 10\n",
      "train loss: 0.431755  [   16/141120]\n",
      "train loss: 0.240460  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.358522 \n",
      "\n",
      "epoch: 11\n",
      "train loss: 0.541288  [   16/141120]\n",
      "train loss: 0.282783  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 78.4%, Avg loss: 0.355858 \n",
      "\n",
      "epoch: 12\n",
      "train loss: 0.360879  [   16/141120]\n",
      "train loss: 0.452308  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.358827 \n",
      "\n",
      "epoch: 13\n",
      "train loss: 0.534352  [   16/141120]\n",
      "train loss: 0.368838  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 78.2%, Avg loss: 0.353938 \n",
      "\n",
      "epoch: 14\n",
      "train loss: 0.291930  [   16/141120]\n",
      "train loss: 0.364881  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 78.4%, Avg loss: 0.355616 \n",
      "\n",
      "=========================\n",
      "Trials from Best to Worst\n",
      "=========================\n",
      "\n",
      "1: Trial #1, Accuracy = 81.7063492063492, Avg Test Loss = 0.33433154225349426, Hidden Size = 64, Num Layers = 2, Bias = False, Batch First = True, Dropout = 0.0, Bidirectional = True, Num Epochs = 13, Learning Rate = 0.01, Batch Size = 16\n",
      "2: Trial #4, Accuracy = 78.44104308390023, Avg Test Loss = 0.3556157946586609, Hidden Size = 32, Num Layers = 4, Bias = False, Batch First = False, Dropout = 0.0, Bidirectional = True, Num Epochs = 15, Learning Rate = 0.01, Batch Size = 16\n",
      "3: Trial #2, Accuracy = 70.94954648526077, Avg Test Loss = 0.5936791300773621, Hidden Size = 64, Num Layers = 4, Bias = False, Batch First = True, Dropout = 0.0, Bidirectional = True, Num Epochs = 10, Learning Rate = 0.001, Batch Size = 16\n",
      "4: Trial #3, Accuracy = 70.65192743764173, Avg Test Loss = 0.5930311679840088, Hidden Size = 32, Num Layers = 3, Bias = False, Batch First = False, Dropout = 0.0, Bidirectional = False, Num Epochs = 11, Learning Rate = 0.001, Batch Size = 16\n",
      "============================================================================================== \n",
      "Trial 5 \n",
      "\n",
      "hidden_size:  32 \n",
      "num_layers:  2 \n",
      "bias =  True \n",
      "batch_first =  False \n",
      "dropout =  0.0 \n",
      "bidirectional =  False \n",
      "num_epochs =  10 \n",
      "learning_rate =  0.01 \n",
      "batch_size =  16 \n",
      "=========================\n",
      "epoch: 0\n",
      "train loss: 0.740253  [   16/141120]\n",
      "train loss: 0.580455  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.595323 \n",
      "\n",
      "epoch: 1\n",
      "train loss: 0.380165  [   16/141120]\n",
      "train loss: 0.555065  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.597181 \n",
      "\n",
      "epoch: 2\n",
      "train loss: 0.566803  [   16/141120]\n",
      "train loss: 0.606020  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.591806 \n",
      "\n",
      "epoch: 3\n",
      "train loss: 0.605812  [   16/141120]\n",
      "train loss: 0.743655  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.593188 \n",
      "\n",
      "epoch: 4\n",
      "train loss: 0.609922  [   16/141120]\n",
      "train loss: 0.446073  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.590528 \n",
      "\n",
      "epoch: 5\n",
      "train loss: 0.659262  [   16/141120]\n",
      "train loss: 0.459002  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 0.584793 \n",
      "\n",
      "epoch: 6\n",
      "train loss: 0.452921  [   16/141120]\n",
      "train loss: 0.507505  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 0.570838 \n",
      "\n",
      "epoch: 7\n",
      "train loss: 0.476606  [   16/141120]\n",
      "train loss: 0.665950  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 0.503914 \n",
      "\n",
      "epoch: 8\n",
      "train loss: 0.504923  [   16/141120]\n",
      "train loss: 0.310100  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.401688 \n",
      "\n",
      "epoch: 9\n",
      "train loss: 0.431368  [   16/141120]\n",
      "train loss: 0.341219  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 77.6%, Avg loss: 0.376347 \n",
      "\n",
      "=========================\n",
      "Trials from Best to Worst\n",
      "=========================\n",
      "\n",
      "1: Trial #1, Accuracy = 81.7063492063492, Avg Test Loss = 0.33433154225349426, Hidden Size = 64, Num Layers = 2, Bias = False, Batch First = True, Dropout = 0.0, Bidirectional = True, Num Epochs = 13, Learning Rate = 0.01, Batch Size = 16\n",
      "2: Trial #4, Accuracy = 78.44104308390023, Avg Test Loss = 0.3556157946586609, Hidden Size = 32, Num Layers = 4, Bias = False, Batch First = False, Dropout = 0.0, Bidirectional = True, Num Epochs = 15, Learning Rate = 0.01, Batch Size = 16\n",
      "3: Trial #5, Accuracy = 77.5907029478458, Avg Test Loss = 0.37634673714637756, Hidden Size = 32, Num Layers = 2, Bias = True, Batch First = False, Dropout = 0.0, Bidirectional = False, Num Epochs = 10, Learning Rate = 0.01, Batch Size = 16\n",
      "4: Trial #2, Accuracy = 70.94954648526077, Avg Test Loss = 0.5936791300773621, Hidden Size = 64, Num Layers = 4, Bias = False, Batch First = True, Dropout = 0.0, Bidirectional = True, Num Epochs = 10, Learning Rate = 0.001, Batch Size = 16\n",
      "5: Trial #3, Accuracy = 70.65192743764173, Avg Test Loss = 0.5930311679840088, Hidden Size = 32, Num Layers = 3, Bias = False, Batch First = False, Dropout = 0.0, Bidirectional = False, Num Epochs = 11, Learning Rate = 0.001, Batch Size = 16\n",
      "============================================================================================== \n",
      "Trial 6 \n",
      "\n",
      "hidden_size:  64 \n",
      "num_layers:  5 \n",
      "bias =  False \n",
      "batch_first =  False \n",
      "dropout =  0.0 \n",
      "bidirectional =  False \n",
      "num_epochs =  14 \n",
      "learning_rate =  0.01 \n",
      "batch_size =  16 \n",
      "=========================\n",
      "epoch: 0\n",
      "train loss: 0.779716  [   16/141120]\n",
      "train loss: 0.565936  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.602523 \n",
      "\n",
      "epoch: 1\n",
      "train loss: 0.499378  [   16/141120]\n",
      "train loss: 0.478755  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.591974 \n",
      "\n",
      "epoch: 2\n",
      "train loss: 0.704108  [   16/141120]\n",
      "train loss: 0.585127  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.590607 \n",
      "\n",
      "epoch: 3\n",
      "train loss: 0.503575  [   16/141120]\n",
      "train loss: 0.474358  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.584511 \n",
      "\n",
      "epoch: 4\n",
      "train loss: 0.542160  [   16/141120]\n",
      "train loss: 0.384698  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 0.575071 \n",
      "\n",
      "epoch: 5\n",
      "train loss: 0.572544  [   16/141120]\n",
      "train loss: 0.574096  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.571273 \n",
      "\n",
      "epoch: 6\n",
      "train loss: 0.722788  [   16/141120]\n",
      "train loss: 0.502261  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 72.3%, Avg loss: 0.538236 \n",
      "\n",
      "epoch: 7\n",
      "train loss: 0.612747  [   16/141120]\n",
      "train loss: 0.241445  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.439202 \n",
      "\n",
      "epoch: 8\n",
      "train loss: 0.559075  [   16/141120]\n",
      "train loss: 0.388828  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.373976 \n",
      "\n",
      "epoch: 9\n",
      "train loss: 0.375515  [   16/141120]\n",
      "train loss: 0.367247  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 79.0%, Avg loss: 0.354965 \n",
      "\n",
      "epoch: 10\n",
      "train loss: 0.353792  [   16/141120]\n",
      "train loss: 0.305742  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 80.0%, Avg loss: 0.340004 \n",
      "\n",
      "epoch: 11\n",
      "train loss: 0.304021  [   16/141120]\n",
      "train loss: 0.162680  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 82.4%, Avg loss: 0.329299 \n",
      "\n",
      "epoch: 12\n",
      "train loss: 0.378651  [   16/141120]\n",
      "train loss: 0.358505  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 83.4%, Avg loss: 0.310972 \n",
      "\n",
      "epoch: 13\n",
      "train loss: 0.297439  [   16/141120]\n",
      "train loss: 0.363980  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 84.6%, Avg loss: 0.293568 \n",
      "\n",
      "=========================\n",
      "Trials from Best to Worst\n",
      "=========================\n",
      "\n",
      "1: Trial #6, Accuracy = 84.64569160997732, Avg Test Loss = 0.2935680150985718, Hidden Size = 64, Num Layers = 5, Bias = False, Batch First = False, Dropout = 0.0, Bidirectional = False, Num Epochs = 14, Learning Rate = 0.01, Batch Size = 16\n",
      "2: Trial #1, Accuracy = 81.7063492063492, Avg Test Loss = 0.33433154225349426, Hidden Size = 64, Num Layers = 2, Bias = False, Batch First = True, Dropout = 0.0, Bidirectional = True, Num Epochs = 13, Learning Rate = 0.01, Batch Size = 16\n",
      "3: Trial #4, Accuracy = 78.44104308390023, Avg Test Loss = 0.3556157946586609, Hidden Size = 32, Num Layers = 4, Bias = False, Batch First = False, Dropout = 0.0, Bidirectional = True, Num Epochs = 15, Learning Rate = 0.01, Batch Size = 16\n",
      "4: Trial #5, Accuracy = 77.5907029478458, Avg Test Loss = 0.37634673714637756, Hidden Size = 32, Num Layers = 2, Bias = True, Batch First = False, Dropout = 0.0, Bidirectional = False, Num Epochs = 10, Learning Rate = 0.01, Batch Size = 16\n",
      "5: Trial #2, Accuracy = 70.94954648526077, Avg Test Loss = 0.5936791300773621, Hidden Size = 64, Num Layers = 4, Bias = False, Batch First = True, Dropout = 0.0, Bidirectional = True, Num Epochs = 10, Learning Rate = 0.001, Batch Size = 16\n",
      "6: Trial #3, Accuracy = 70.65192743764173, Avg Test Loss = 0.5930311679840088, Hidden Size = 32, Num Layers = 3, Bias = False, Batch First = False, Dropout = 0.0, Bidirectional = False, Num Epochs = 11, Learning Rate = 0.001, Batch Size = 16\n",
      "============================================================================================== \n",
      "Trial 7 \n",
      "\n",
      "hidden_size:  64 \n",
      "num_layers:  2 \n",
      "bias =  False \n",
      "batch_first =  False \n",
      "dropout =  0.0 \n",
      "bidirectional =  True \n",
      "num_epochs =  11 \n",
      "learning_rate =  0.001 \n",
      "batch_size =  16 \n",
      "=========================\n",
      "epoch: 0\n",
      "train loss: 0.698608  [   16/141120]\n",
      "train loss: 0.467511  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.604797 \n",
      "\n",
      "epoch: 1\n",
      "train loss: 0.621268  [   16/141120]\n",
      "train loss: 0.573637  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.601384 \n",
      "\n",
      "epoch: 2\n",
      "train loss: 0.785884  [   16/141120]\n",
      "train loss: 0.562221  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.598726 \n",
      "\n",
      "epoch: 3\n",
      "train loss: 0.686095  [   16/141120]\n",
      "train loss: 0.631572  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.597026 \n",
      "\n",
      "epoch: 4\n",
      "train loss: 0.517191  [   16/141120]\n",
      "train loss: 0.546683  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.596039 \n",
      "\n",
      "epoch: 5\n",
      "train loss: 0.714382  [   16/141120]\n",
      "train loss: 0.627049  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 0.596165 \n",
      "\n",
      "epoch: 6\n",
      "train loss: 0.731851  [   16/141120]\n",
      "train loss: 0.579095  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.595906 \n",
      "\n",
      "epoch: 7\n",
      "train loss: 0.798358  [   16/141120]\n",
      "train loss: 0.706855  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 0.595442 \n",
      "\n",
      "epoch: 8\n",
      "train loss: 0.561052  [   16/141120]\n",
      "train loss: 0.710989  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 0.595344 \n",
      "\n",
      "epoch: 9\n",
      "train loss: 0.608734  [   16/141120]\n",
      "train loss: 0.685865  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.595283 \n",
      "\n",
      "epoch: 10\n",
      "train loss: 0.539970  [   16/141120]\n",
      "train loss: 0.498224  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.595195 \n",
      "\n",
      "=========================\n",
      "Trials from Best to Worst\n",
      "=========================\n",
      "\n",
      "1: Trial #6, Accuracy = 84.64569160997732, Avg Test Loss = 0.2935680150985718, Hidden Size = 64, Num Layers = 5, Bias = False, Batch First = False, Dropout = 0.0, Bidirectional = False, Num Epochs = 14, Learning Rate = 0.01, Batch Size = 16\n",
      "2: Trial #1, Accuracy = 81.7063492063492, Avg Test Loss = 0.33433154225349426, Hidden Size = 64, Num Layers = 2, Bias = False, Batch First = True, Dropout = 0.0, Bidirectional = True, Num Epochs = 13, Learning Rate = 0.01, Batch Size = 16\n",
      "3: Trial #4, Accuracy = 78.44104308390023, Avg Test Loss = 0.3556157946586609, Hidden Size = 32, Num Layers = 4, Bias = False, Batch First = False, Dropout = 0.0, Bidirectional = True, Num Epochs = 15, Learning Rate = 0.01, Batch Size = 16\n",
      "4: Trial #5, Accuracy = 77.5907029478458, Avg Test Loss = 0.37634673714637756, Hidden Size = 32, Num Layers = 2, Bias = True, Batch First = False, Dropout = 0.0, Bidirectional = False, Num Epochs = 10, Learning Rate = 0.01, Batch Size = 16\n",
      "5: Trial #2, Accuracy = 70.94954648526077, Avg Test Loss = 0.5936791300773621, Hidden Size = 64, Num Layers = 4, Bias = False, Batch First = True, Dropout = 0.0, Bidirectional = True, Num Epochs = 10, Learning Rate = 0.001, Batch Size = 16\n",
      "6: Trial #7, Accuracy = 70.66043083900226, Avg Test Loss = 0.5951948761940002, Hidden Size = 64, Num Layers = 2, Bias = False, Batch First = False, Dropout = 0.0, Bidirectional = True, Num Epochs = 11, Learning Rate = 0.001, Batch Size = 16\n",
      "7: Trial #3, Accuracy = 70.65192743764173, Avg Test Loss = 0.5930311679840088, Hidden Size = 32, Num Layers = 3, Bias = False, Batch First = False, Dropout = 0.0, Bidirectional = False, Num Epochs = 11, Learning Rate = 0.001, Batch Size = 16\n",
      "============================================================================================== \n",
      "Trial 8 \n",
      "\n",
      "hidden_size:  128 \n",
      "num_layers:  4 \n",
      "bias =  True \n",
      "batch_first =  False \n",
      "dropout =  0.0 \n",
      "bidirectional =  False \n",
      "num_epochs =  14 \n",
      "learning_rate =  0.001 \n",
      "batch_size =  16 \n",
      "=========================\n",
      "epoch: 0\n",
      "train loss: 0.685742  [   16/141120]\n",
      "train loss: 0.623519  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.605034 \n",
      "\n",
      "epoch: 1\n",
      "train loss: 0.727958  [   16/141120]\n",
      "train loss: 0.618678  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.604643 \n",
      "\n",
      "epoch: 2\n",
      "train loss: 0.721910  [   16/141120]\n",
      "train loss: 0.466367  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.604186 \n",
      "\n",
      "epoch: 3\n",
      "train loss: 0.835374  [   16/141120]\n",
      "train loss: 0.670590  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.603698 \n",
      "\n",
      "epoch: 4\n",
      "train loss: 0.731579  [   16/141120]\n",
      "train loss: 0.619554  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.602948 \n",
      "\n",
      "epoch: 5\n",
      "train loss: 0.629278  [   16/141120]\n",
      "train loss: 0.736062  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.602008 \n",
      "\n",
      "epoch: 6\n",
      "train loss: 0.459406  [   16/141120]\n",
      "train loss: 0.814958  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.600570 \n",
      "\n",
      "epoch: 7\n",
      "train loss: 0.551045  [   16/141120]\n",
      "train loss: 0.492532  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.599084 \n",
      "\n",
      "epoch: 8\n",
      "train loss: 0.434996  [   16/141120]\n",
      "train loss: 0.606217  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.597463 \n",
      "\n",
      "epoch: 9\n",
      "train loss: 0.591029  [   16/141120]\n",
      "train loss: 0.621651  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.595458 \n",
      "\n",
      "epoch: 10\n",
      "train loss: 0.650684  [   16/141120]\n",
      "train loss: 0.808813  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.593991 \n",
      "\n",
      "epoch: 11\n",
      "train loss: 0.529315  [   16/141120]\n",
      "train loss: 0.588883  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.592758 \n",
      "\n",
      "epoch: 12\n",
      "train loss: 0.642645  [   16/141120]\n",
      "train loss: 0.543991  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.592554 \n",
      "\n",
      "epoch: 13\n",
      "train loss: 0.583095  [   16/141120]\n",
      "train loss: 0.630472  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.592334 \n",
      "\n",
      "=========================\n",
      "Trials from Best to Worst\n",
      "=========================\n",
      "\n",
      "1: Trial #6, Accuracy = 84.64569160997732, Avg Test Loss = 0.2935680150985718, Hidden Size = 64, Num Layers = 5, Bias = False, Batch First = False, Dropout = 0.0, Bidirectional = False, Num Epochs = 14, Learning Rate = 0.01, Batch Size = 16\n",
      "2: Trial #1, Accuracy = 81.7063492063492, Avg Test Loss = 0.33433154225349426, Hidden Size = 64, Num Layers = 2, Bias = False, Batch First = True, Dropout = 0.0, Bidirectional = True, Num Epochs = 13, Learning Rate = 0.01, Batch Size = 16\n",
      "3: Trial #4, Accuracy = 78.44104308390023, Avg Test Loss = 0.3556157946586609, Hidden Size = 32, Num Layers = 4, Bias = False, Batch First = False, Dropout = 0.0, Bidirectional = True, Num Epochs = 15, Learning Rate = 0.01, Batch Size = 16\n",
      "4: Trial #5, Accuracy = 77.5907029478458, Avg Test Loss = 0.37634673714637756, Hidden Size = 32, Num Layers = 2, Bias = True, Batch First = False, Dropout = 0.0, Bidirectional = False, Num Epochs = 10, Learning Rate = 0.01, Batch Size = 16\n",
      "5: Trial #2, Accuracy = 70.94954648526077, Avg Test Loss = 0.5936791300773621, Hidden Size = 64, Num Layers = 4, Bias = False, Batch First = True, Dropout = 0.0, Bidirectional = True, Num Epochs = 10, Learning Rate = 0.001, Batch Size = 16\n",
      "6: Trial #7, Accuracy = 70.66043083900226, Avg Test Loss = 0.5951948761940002, Hidden Size = 64, Num Layers = 2, Bias = False, Batch First = False, Dropout = 0.0, Bidirectional = True, Num Epochs = 11, Learning Rate = 0.001, Batch Size = 16\n",
      "7: Trial #3, Accuracy = 70.65192743764173, Avg Test Loss = 0.5930311679840088, Hidden Size = 32, Num Layers = 3, Bias = False, Batch First = False, Dropout = 0.0, Bidirectional = False, Num Epochs = 11, Learning Rate = 0.001, Batch Size = 16\n",
      "8: Trial #8, Accuracy = 70.65192743764173, Avg Test Loss = 0.5923343300819397, Hidden Size = 128, Num Layers = 4, Bias = True, Batch First = False, Dropout = 0.0, Bidirectional = False, Num Epochs = 14, Learning Rate = 0.001, Batch Size = 16\n",
      "============================================================================================== \n",
      "Trial 9 \n",
      "\n",
      "hidden_size:  256 \n",
      "num_layers:  4 \n",
      "bias =  False \n",
      "batch_first =  True \n",
      "dropout =  0.0 \n",
      "bidirectional =  False \n",
      "num_epochs =  13 \n",
      "learning_rate =  0.001 \n",
      "batch_size =  16 \n",
      "=========================\n",
      "epoch: 0\n",
      "train loss: 0.691770  [   16/141120]\n",
      "train loss: 0.519862  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.605211 \n",
      "\n",
      "epoch: 1\n",
      "train loss: 0.669692  [   16/141120]\n",
      "train loss: 0.668141  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.604791 \n",
      "\n",
      "epoch: 2\n",
      "train loss: 0.614862  [   16/141120]\n",
      "train loss: 0.620841  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.604263 \n",
      "\n",
      "epoch: 3\n",
      "train loss: 0.678283  [   16/141120]\n",
      "train loss: 0.572559  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.603751 \n",
      "\n",
      "epoch: 4\n",
      "train loss: 0.622249  [   16/141120]\n",
      "train loss: 0.837939  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.602963 \n",
      "\n",
      "epoch: 5\n",
      "train loss: 0.618133  [   16/141120]\n",
      "train loss: 0.666276  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.601985 \n",
      "\n",
      "epoch: 6\n",
      "train loss: 0.684126  [   16/141120]\n",
      "train loss: 0.836147  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.600406 \n",
      "\n",
      "epoch: 7\n",
      "train loss: 0.609535  [   16/141120]\n",
      "train loss: 0.607951  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.598517 \n",
      "\n",
      "epoch: 8\n",
      "train loss: 0.809890  [   16/141120]\n",
      "train loss: 0.616818  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.596325 \n",
      "\n",
      "epoch: 9\n",
      "train loss: 0.669973  [   16/141120]\n",
      "train loss: 0.604337  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.594269 \n",
      "\n",
      "epoch: 10\n",
      "train loss: 0.697874  [   16/141120]\n",
      "train loss: 0.572706  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.593066 \n",
      "\n",
      "epoch: 11\n",
      "train loss: 0.666044  [   16/141120]\n",
      "train loss: 0.860022  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.592521 \n",
      "\n",
      "epoch: 12\n",
      "train loss: 0.650864  [   16/141120]\n",
      "train loss: 0.507059  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.592202 \n",
      "\n",
      "=========================\n",
      "Trials from Best to Worst\n",
      "=========================\n",
      "\n",
      "1: Trial #6, Accuracy = 84.64569160997732, Avg Test Loss = 0.2935680150985718, Hidden Size = 64, Num Layers = 5, Bias = False, Batch First = False, Dropout = 0.0, Bidirectional = False, Num Epochs = 14, Learning Rate = 0.01, Batch Size = 16\n",
      "2: Trial #1, Accuracy = 81.7063492063492, Avg Test Loss = 0.33433154225349426, Hidden Size = 64, Num Layers = 2, Bias = False, Batch First = True, Dropout = 0.0, Bidirectional = True, Num Epochs = 13, Learning Rate = 0.01, Batch Size = 16\n",
      "3: Trial #4, Accuracy = 78.44104308390023, Avg Test Loss = 0.3556157946586609, Hidden Size = 32, Num Layers = 4, Bias = False, Batch First = False, Dropout = 0.0, Bidirectional = True, Num Epochs = 15, Learning Rate = 0.01, Batch Size = 16\n",
      "4: Trial #5, Accuracy = 77.5907029478458, Avg Test Loss = 0.37634673714637756, Hidden Size = 32, Num Layers = 2, Bias = True, Batch First = False, Dropout = 0.0, Bidirectional = False, Num Epochs = 10, Learning Rate = 0.01, Batch Size = 16\n",
      "5: Trial #2, Accuracy = 70.94954648526077, Avg Test Loss = 0.5936791300773621, Hidden Size = 64, Num Layers = 4, Bias = False, Batch First = True, Dropout = 0.0, Bidirectional = True, Num Epochs = 10, Learning Rate = 0.001, Batch Size = 16\n",
      "6: Trial #7, Accuracy = 70.66043083900226, Avg Test Loss = 0.5951948761940002, Hidden Size = 64, Num Layers = 2, Bias = False, Batch First = False, Dropout = 0.0, Bidirectional = True, Num Epochs = 11, Learning Rate = 0.001, Batch Size = 16\n",
      "7: Trial #3, Accuracy = 70.65192743764173, Avg Test Loss = 0.5930311679840088, Hidden Size = 32, Num Layers = 3, Bias = False, Batch First = False, Dropout = 0.0, Bidirectional = False, Num Epochs = 11, Learning Rate = 0.001, Batch Size = 16\n",
      "8: Trial #8, Accuracy = 70.65192743764173, Avg Test Loss = 0.5923343300819397, Hidden Size = 128, Num Layers = 4, Bias = True, Batch First = False, Dropout = 0.0, Bidirectional = False, Num Epochs = 14, Learning Rate = 0.001, Batch Size = 16\n",
      "9: Trial #9, Accuracy = 70.65192743764173, Avg Test Loss = 0.5922024846076965, Hidden Size = 256, Num Layers = 4, Bias = False, Batch First = True, Dropout = 0.0, Bidirectional = False, Num Epochs = 13, Learning Rate = 0.001, Batch Size = 16\n",
      "============================================================================================== \n",
      "Trial 10 \n",
      "\n",
      "hidden_size:  256 \n",
      "num_layers:  3 \n",
      "bias =  True \n",
      "batch_first =  True \n",
      "dropout =  0.0 \n",
      "bidirectional =  True \n",
      "num_epochs =  10 \n",
      "learning_rate =  0.01 \n",
      "batch_size =  16 \n",
      "=========================\n",
      "epoch: 0\n",
      "train loss: 0.702591  [   16/141120]\n",
      "train loss: 0.785938  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 0.595365 \n",
      "\n",
      "epoch: 1\n",
      "train loss: 0.422020  [   16/141120]\n",
      "train loss: 0.729505  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.598650 \n",
      "\n",
      "epoch: 2\n",
      "train loss: 0.619521  [   16/141120]\n",
      "train loss: 0.492146  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.593247 \n",
      "\n",
      "epoch: 3\n",
      "train loss: 0.672508  [   16/141120]\n",
      "train loss: 0.447304  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.594066 \n",
      "\n",
      "epoch: 4\n",
      "train loss: 0.558815  [   16/141120]\n",
      "train loss: 0.437792  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.590540 \n",
      "\n",
      "epoch: 5\n",
      "train loss: 0.420856  [   16/141120]\n",
      "train loss: 0.587716  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.583907 \n",
      "\n",
      "epoch: 6\n",
      "train loss: 0.493459  [   16/141120]\n",
      "train loss: 0.562295  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.570040 \n",
      "\n",
      "epoch: 7\n",
      "train loss: 0.610650  [   16/141120]\n",
      "train loss: 0.461439  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 0.570890 \n",
      "\n",
      "epoch: 8\n",
      "train loss: 0.607992  [   16/141120]\n",
      "train loss: 0.623584  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 71.3%, Avg loss: 0.564597 \n",
      "\n",
      "epoch: 9\n",
      "train loss: 0.492173  [   16/141120]\n",
      "train loss: 0.670470  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 71.6%, Avg loss: 0.548804 \n",
      "\n",
      "=========================\n",
      "Trials from Best to Worst\n",
      "=========================\n",
      "\n",
      "1: Trial #6, Accuracy = 84.64569160997732, Avg Test Loss = 0.2935680150985718, Hidden Size = 64, Num Layers = 5, Bias = False, Batch First = False, Dropout = 0.0, Bidirectional = False, Num Epochs = 14, Learning Rate = 0.01, Batch Size = 16\n",
      "2: Trial #1, Accuracy = 81.7063492063492, Avg Test Loss = 0.33433154225349426, Hidden Size = 64, Num Layers = 2, Bias = False, Batch First = True, Dropout = 0.0, Bidirectional = True, Num Epochs = 13, Learning Rate = 0.01, Batch Size = 16\n",
      "3: Trial #4, Accuracy = 78.44104308390023, Avg Test Loss = 0.3556157946586609, Hidden Size = 32, Num Layers = 4, Bias = False, Batch First = False, Dropout = 0.0, Bidirectional = True, Num Epochs = 15, Learning Rate = 0.01, Batch Size = 16\n",
      "4: Trial #5, Accuracy = 77.5907029478458, Avg Test Loss = 0.37634673714637756, Hidden Size = 32, Num Layers = 2, Bias = True, Batch First = False, Dropout = 0.0, Bidirectional = False, Num Epochs = 10, Learning Rate = 0.01, Batch Size = 16\n",
      "5: Trial #10, Accuracy = 71.57879818594104, Avg Test Loss = 0.5488039255142212, Hidden Size = 256, Num Layers = 3, Bias = True, Batch First = True, Dropout = 0.0, Bidirectional = True, Num Epochs = 10, Learning Rate = 0.01, Batch Size = 16\n",
      "6: Trial #2, Accuracy = 70.94954648526077, Avg Test Loss = 0.5936791300773621, Hidden Size = 64, Num Layers = 4, Bias = False, Batch First = True, Dropout = 0.0, Bidirectional = True, Num Epochs = 10, Learning Rate = 0.001, Batch Size = 16\n",
      "7: Trial #7, Accuracy = 70.66043083900226, Avg Test Loss = 0.5951948761940002, Hidden Size = 64, Num Layers = 2, Bias = False, Batch First = False, Dropout = 0.0, Bidirectional = True, Num Epochs = 11, Learning Rate = 0.001, Batch Size = 16\n",
      "8: Trial #3, Accuracy = 70.65192743764173, Avg Test Loss = 0.5930311679840088, Hidden Size = 32, Num Layers = 3, Bias = False, Batch First = False, Dropout = 0.0, Bidirectional = False, Num Epochs = 11, Learning Rate = 0.001, Batch Size = 16\n",
      "9: Trial #8, Accuracy = 70.65192743764173, Avg Test Loss = 0.5923343300819397, Hidden Size = 128, Num Layers = 4, Bias = True, Batch First = False, Dropout = 0.0, Bidirectional = False, Num Epochs = 14, Learning Rate = 0.001, Batch Size = 16\n",
      "10: Trial #9, Accuracy = 70.65192743764173, Avg Test Loss = 0.5922024846076965, Hidden Size = 256, Num Layers = 4, Bias = False, Batch First = True, Dropout = 0.0, Bidirectional = False, Num Epochs = 13, Learning Rate = 0.001, Batch Size = 16\n",
      "============================================================================================== \n",
      "Trial 11 \n",
      "\n",
      "hidden_size:  64 \n",
      "num_layers:  2 \n",
      "bias =  False \n",
      "batch_first =  True \n",
      "dropout =  0.0 \n",
      "bidirectional =  False \n",
      "num_epochs =  10 \n",
      "learning_rate =  0.01 \n",
      "batch_size =  16 \n",
      "=========================\n",
      "epoch: 0\n",
      "train loss: 0.706210  [   16/141120]\n",
      "train loss: 0.612620  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.594134 \n",
      "\n",
      "epoch: 1\n",
      "train loss: 0.493571  [   16/141120]\n",
      "train loss: 0.672159  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.592531 \n",
      "\n",
      "epoch: 2\n",
      "train loss: 0.645091  [   16/141120]\n",
      "train loss: 0.792327  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 0.591429 \n",
      "\n",
      "epoch: 3\n",
      "train loss: 0.519923  [   16/141120]\n",
      "train loss: 0.292732  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.594785 \n",
      "\n",
      "epoch: 4\n",
      "train loss: 0.624922  [   16/141120]\n",
      "train loss: 0.625167  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.588732 \n",
      "\n",
      "epoch: 5\n",
      "train loss: 0.519636  [   16/141120]\n",
      "train loss: 0.572694  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.579176 \n",
      "\n",
      "epoch: 6\n",
      "train loss: 0.561552  [   16/141120]\n",
      "train loss: 0.588850  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Avg loss: 0.519113 \n",
      "\n",
      "epoch: 7\n",
      "train loss: 0.638353  [   16/141120]\n",
      "train loss: 0.277238  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 76.6%, Avg loss: 0.420515 \n",
      "\n",
      "epoch: 8\n",
      "train loss: 0.425367  [   16/141120]\n",
      "train loss: 0.549436  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 77.7%, Avg loss: 0.382799 \n",
      "\n",
      "epoch: 9\n",
      "train loss: 0.448999  [   16/141120]\n",
      "train loss: 0.364058  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 79.8%, Avg loss: 0.356329 \n",
      "\n",
      "=========================\n",
      "Trials from Best to Worst\n",
      "=========================\n",
      "\n",
      "1: Trial #6, Accuracy = 84.64569160997732, Avg Test Loss = 0.2935680150985718, Hidden Size = 64, Num Layers = 5, Bias = False, Batch First = False, Dropout = 0.0, Bidirectional = False, Num Epochs = 14, Learning Rate = 0.01, Batch Size = 16\n",
      "2: Trial #1, Accuracy = 81.7063492063492, Avg Test Loss = 0.33433154225349426, Hidden Size = 64, Num Layers = 2, Bias = False, Batch First = True, Dropout = 0.0, Bidirectional = True, Num Epochs = 13, Learning Rate = 0.01, Batch Size = 16\n",
      "3: Trial #11, Accuracy = 79.77607709750568, Avg Test Loss = 0.3563286066055298, Hidden Size = 64, Num Layers = 2, Bias = False, Batch First = True, Dropout = 0.0, Bidirectional = False, Num Epochs = 10, Learning Rate = 0.01, Batch Size = 16\n",
      "4: Trial #4, Accuracy = 78.44104308390023, Avg Test Loss = 0.3556157946586609, Hidden Size = 32, Num Layers = 4, Bias = False, Batch First = False, Dropout = 0.0, Bidirectional = True, Num Epochs = 15, Learning Rate = 0.01, Batch Size = 16\n",
      "5: Trial #5, Accuracy = 77.5907029478458, Avg Test Loss = 0.37634673714637756, Hidden Size = 32, Num Layers = 2, Bias = True, Batch First = False, Dropout = 0.0, Bidirectional = False, Num Epochs = 10, Learning Rate = 0.01, Batch Size = 16\n",
      "6: Trial #10, Accuracy = 71.57879818594104, Avg Test Loss = 0.5488039255142212, Hidden Size = 256, Num Layers = 3, Bias = True, Batch First = True, Dropout = 0.0, Bidirectional = True, Num Epochs = 10, Learning Rate = 0.01, Batch Size = 16\n",
      "7: Trial #2, Accuracy = 70.94954648526077, Avg Test Loss = 0.5936791300773621, Hidden Size = 64, Num Layers = 4, Bias = False, Batch First = True, Dropout = 0.0, Bidirectional = True, Num Epochs = 10, Learning Rate = 0.001, Batch Size = 16\n",
      "8: Trial #7, Accuracy = 70.66043083900226, Avg Test Loss = 0.5951948761940002, Hidden Size = 64, Num Layers = 2, Bias = False, Batch First = False, Dropout = 0.0, Bidirectional = True, Num Epochs = 11, Learning Rate = 0.001, Batch Size = 16\n",
      "9: Trial #3, Accuracy = 70.65192743764173, Avg Test Loss = 0.5930311679840088, Hidden Size = 32, Num Layers = 3, Bias = False, Batch First = False, Dropout = 0.0, Bidirectional = False, Num Epochs = 11, Learning Rate = 0.001, Batch Size = 16\n",
      "10: Trial #8, Accuracy = 70.65192743764173, Avg Test Loss = 0.5923343300819397, Hidden Size = 128, Num Layers = 4, Bias = True, Batch First = False, Dropout = 0.0, Bidirectional = False, Num Epochs = 14, Learning Rate = 0.001, Batch Size = 16\n",
      "11: Trial #9, Accuracy = 70.65192743764173, Avg Test Loss = 0.5922024846076965, Hidden Size = 256, Num Layers = 4, Bias = False, Batch First = True, Dropout = 0.0, Bidirectional = False, Num Epochs = 13, Learning Rate = 0.001, Batch Size = 16\n",
      "============================================================================================== \n",
      "Trial 12 \n",
      "\n",
      "hidden_size:  64 \n",
      "num_layers:  3 \n",
      "bias =  False \n",
      "batch_first =  True \n",
      "dropout =  0.0 \n",
      "bidirectional =  True \n",
      "num_epochs =  11 \n",
      "learning_rate =  0.01 \n",
      "batch_size =  16 \n",
      "=========================\n",
      "epoch: 0\n",
      "train loss: 0.725519  [   16/141120]\n",
      "train loss: 0.569973  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.594268 \n",
      "\n",
      "epoch: 1\n",
      "train loss: 0.520543  [   16/141120]\n",
      "train loss: 0.675847  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.593225 \n",
      "\n",
      "epoch: 2\n",
      "train loss: 0.855432  [   16/141120]\n",
      "train loss: 0.626567  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 0.593749 \n",
      "\n",
      "epoch: 3\n",
      "train loss: 0.697867  [   16/141120]\n",
      "train loss: 0.603803  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.591416 \n",
      "\n",
      "epoch: 4\n",
      "train loss: 0.452012  [   16/141120]\n",
      "train loss: 0.466012  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.590421 \n",
      "\n",
      "epoch: 5\n",
      "train loss: 0.533903  [   16/141120]\n",
      "train loss: 0.623978  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.586503 \n",
      "\n",
      "epoch: 6\n",
      "train loss: 0.686750  [   16/141120]\n",
      "train loss: 0.726568  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 71.4%, Avg loss: 0.558880 \n",
      "\n",
      "epoch: 7\n",
      "train loss: 0.590161  [   16/141120]\n",
      "train loss: 0.682108  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 74.0%, Avg loss: 0.496999 \n",
      "\n",
      "epoch: 8\n",
      "train loss: 0.369371  [   16/141120]\n",
      "train loss: 0.255511  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 76.7%, Avg loss: 0.440324 \n",
      "\n",
      "epoch: 9\n",
      "train loss: 0.476656  [   16/141120]\n",
      "train loss: 0.364776  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 0.382742 \n",
      "\n",
      "epoch: 10\n",
      "train loss: 0.297757  [   16/141120]\n",
      "train loss: 0.267266  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 77.5%, Avg loss: 0.374524 \n",
      "\n",
      "=========================\n",
      "Trials from Best to Worst\n",
      "=========================\n",
      "\n",
      "1: Trial #6, Accuracy = 84.64569160997732, Avg Test Loss = 0.2935680150985718, Hidden Size = 64, Num Layers = 5, Bias = False, Batch First = False, Dropout = 0.0, Bidirectional = False, Num Epochs = 14, Learning Rate = 0.01, Batch Size = 16\n",
      "2: Trial #1, Accuracy = 81.7063492063492, Avg Test Loss = 0.33433154225349426, Hidden Size = 64, Num Layers = 2, Bias = False, Batch First = True, Dropout = 0.0, Bidirectional = True, Num Epochs = 13, Learning Rate = 0.01, Batch Size = 16\n",
      "3: Trial #11, Accuracy = 79.77607709750568, Avg Test Loss = 0.3563286066055298, Hidden Size = 64, Num Layers = 2, Bias = False, Batch First = True, Dropout = 0.0, Bidirectional = False, Num Epochs = 10, Learning Rate = 0.01, Batch Size = 16\n",
      "4: Trial #4, Accuracy = 78.44104308390023, Avg Test Loss = 0.3556157946586609, Hidden Size = 32, Num Layers = 4, Bias = False, Batch First = False, Dropout = 0.0, Bidirectional = True, Num Epochs = 15, Learning Rate = 0.01, Batch Size = 16\n",
      "5: Trial #5, Accuracy = 77.5907029478458, Avg Test Loss = 0.37634673714637756, Hidden Size = 32, Num Layers = 2, Bias = True, Batch First = False, Dropout = 0.0, Bidirectional = False, Num Epochs = 10, Learning Rate = 0.01, Batch Size = 16\n",
      "6: Trial #12, Accuracy = 77.49716553287982, Avg Test Loss = 0.37452423572540283, Hidden Size = 64, Num Layers = 3, Bias = False, Batch First = True, Dropout = 0.0, Bidirectional = True, Num Epochs = 11, Learning Rate = 0.01, Batch Size = 16\n",
      "7: Trial #10, Accuracy = 71.57879818594104, Avg Test Loss = 0.5488039255142212, Hidden Size = 256, Num Layers = 3, Bias = True, Batch First = True, Dropout = 0.0, Bidirectional = True, Num Epochs = 10, Learning Rate = 0.01, Batch Size = 16\n",
      "8: Trial #2, Accuracy = 70.94954648526077, Avg Test Loss = 0.5936791300773621, Hidden Size = 64, Num Layers = 4, Bias = False, Batch First = True, Dropout = 0.0, Bidirectional = True, Num Epochs = 10, Learning Rate = 0.001, Batch Size = 16\n",
      "9: Trial #7, Accuracy = 70.66043083900226, Avg Test Loss = 0.5951948761940002, Hidden Size = 64, Num Layers = 2, Bias = False, Batch First = False, Dropout = 0.0, Bidirectional = True, Num Epochs = 11, Learning Rate = 0.001, Batch Size = 16\n",
      "10: Trial #3, Accuracy = 70.65192743764173, Avg Test Loss = 0.5930311679840088, Hidden Size = 32, Num Layers = 3, Bias = False, Batch First = False, Dropout = 0.0, Bidirectional = False, Num Epochs = 11, Learning Rate = 0.001, Batch Size = 16\n",
      "11: Trial #8, Accuracy = 70.65192743764173, Avg Test Loss = 0.5923343300819397, Hidden Size = 128, Num Layers = 4, Bias = True, Batch First = False, Dropout = 0.0, Bidirectional = False, Num Epochs = 14, Learning Rate = 0.001, Batch Size = 16\n",
      "12: Trial #9, Accuracy = 70.65192743764173, Avg Test Loss = 0.5922024846076965, Hidden Size = 256, Num Layers = 4, Bias = False, Batch First = True, Dropout = 0.0, Bidirectional = False, Num Epochs = 13, Learning Rate = 0.001, Batch Size = 16\n",
      "============================================================================================== \n",
      "Trial 13 \n",
      "\n",
      "hidden_size:  32 \n",
      "num_layers:  4 \n",
      "bias =  True \n",
      "batch_first =  True \n",
      "dropout =  0.0 \n",
      "bidirectional =  True \n",
      "num_epochs =  10 \n",
      "learning_rate =  0.001 \n",
      "batch_size =  16 \n",
      "=========================\n",
      "epoch: 0\n",
      "train loss: 0.669234  [   16/141120]\n",
      "train loss: 0.615191  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.604046 \n",
      "\n",
      "epoch: 1\n",
      "train loss: 0.622419  [   16/141120]\n",
      "train loss: 0.667046  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.602824 \n",
      "\n",
      "epoch: 2\n",
      "train loss: 0.619865  [   16/141120]\n",
      "train loss: 0.565815  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.600917 \n",
      "\n",
      "epoch: 3\n",
      "train loss: 0.408056  [   16/141120]\n",
      "train loss: 0.663504  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.598784 \n",
      "\n",
      "epoch: 4\n",
      "train loss: 0.554897  [   16/141120]\n",
      "train loss: 0.611673  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.596820 \n",
      "\n",
      "epoch: 5\n",
      "train loss: 0.465636  [   16/141120]\n",
      "train loss: 0.617560  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 0.594996 \n",
      "\n",
      "epoch: 6\n",
      "train loss: 0.542517  [   16/141120]\n",
      "train loss: 0.429997  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.595216 \n",
      "\n",
      "epoch: 7\n",
      "train loss: 0.554765  [   16/141120]\n",
      "train loss: 0.572087  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 0.594454 \n",
      "\n",
      "epoch: 8\n",
      "train loss: 0.660619  [   16/141120]\n",
      "train loss: 0.679152  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.594117 \n",
      "\n",
      "epoch: 9\n",
      "train loss: 0.633724  [   16/141120]\n",
      "train loss: 0.441825  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 0.594353 \n",
      "\n",
      "=========================\n",
      "Trials from Best to Worst\n",
      "=========================\n",
      "\n",
      "1: Trial #6, Accuracy = 84.64569160997732, Avg Test Loss = 0.2935680150985718, Hidden Size = 64, Num Layers = 5, Bias = False, Batch First = False, Dropout = 0.0, Bidirectional = False, Num Epochs = 14, Learning Rate = 0.01, Batch Size = 16\n",
      "2: Trial #1, Accuracy = 81.7063492063492, Avg Test Loss = 0.33433154225349426, Hidden Size = 64, Num Layers = 2, Bias = False, Batch First = True, Dropout = 0.0, Bidirectional = True, Num Epochs = 13, Learning Rate = 0.01, Batch Size = 16\n",
      "3: Trial #11, Accuracy = 79.77607709750568, Avg Test Loss = 0.3563286066055298, Hidden Size = 64, Num Layers = 2, Bias = False, Batch First = True, Dropout = 0.0, Bidirectional = False, Num Epochs = 10, Learning Rate = 0.01, Batch Size = 16\n",
      "4: Trial #4, Accuracy = 78.44104308390023, Avg Test Loss = 0.3556157946586609, Hidden Size = 32, Num Layers = 4, Bias = False, Batch First = False, Dropout = 0.0, Bidirectional = True, Num Epochs = 15, Learning Rate = 0.01, Batch Size = 16\n",
      "5: Trial #5, Accuracy = 77.5907029478458, Avg Test Loss = 0.37634673714637756, Hidden Size = 32, Num Layers = 2, Bias = True, Batch First = False, Dropout = 0.0, Bidirectional = False, Num Epochs = 10, Learning Rate = 0.01, Batch Size = 16\n",
      "6: Trial #12, Accuracy = 77.49716553287982, Avg Test Loss = 0.37452423572540283, Hidden Size = 64, Num Layers = 3, Bias = False, Batch First = True, Dropout = 0.0, Bidirectional = True, Num Epochs = 11, Learning Rate = 0.01, Batch Size = 16\n",
      "7: Trial #10, Accuracy = 71.57879818594104, Avg Test Loss = 0.5488039255142212, Hidden Size = 256, Num Layers = 3, Bias = True, Batch First = True, Dropout = 0.0, Bidirectional = True, Num Epochs = 10, Learning Rate = 0.01, Batch Size = 16\n",
      "8: Trial #2, Accuracy = 70.94954648526077, Avg Test Loss = 0.5936791300773621, Hidden Size = 64, Num Layers = 4, Bias = False, Batch First = True, Dropout = 0.0, Bidirectional = True, Num Epochs = 10, Learning Rate = 0.001, Batch Size = 16\n",
      "9: Trial #13, Accuracy = 70.9297052154195, Avg Test Loss = 0.5943533182144165, Hidden Size = 32, Num Layers = 4, Bias = True, Batch First = True, Dropout = 0.0, Bidirectional = True, Num Epochs = 10, Learning Rate = 0.001, Batch Size = 16\n",
      "10: Trial #7, Accuracy = 70.66043083900226, Avg Test Loss = 0.5951948761940002, Hidden Size = 64, Num Layers = 2, Bias = False, Batch First = False, Dropout = 0.0, Bidirectional = True, Num Epochs = 11, Learning Rate = 0.001, Batch Size = 16\n",
      "11: Trial #3, Accuracy = 70.65192743764173, Avg Test Loss = 0.5930311679840088, Hidden Size = 32, Num Layers = 3, Bias = False, Batch First = False, Dropout = 0.0, Bidirectional = False, Num Epochs = 11, Learning Rate = 0.001, Batch Size = 16\n",
      "12: Trial #8, Accuracy = 70.65192743764173, Avg Test Loss = 0.5923343300819397, Hidden Size = 128, Num Layers = 4, Bias = True, Batch First = False, Dropout = 0.0, Bidirectional = False, Num Epochs = 14, Learning Rate = 0.001, Batch Size = 16\n",
      "13: Trial #9, Accuracy = 70.65192743764173, Avg Test Loss = 0.5922024846076965, Hidden Size = 256, Num Layers = 4, Bias = False, Batch First = True, Dropout = 0.0, Bidirectional = False, Num Epochs = 13, Learning Rate = 0.001, Batch Size = 16\n",
      "============================================================================================== \n",
      "Trial 14 \n",
      "\n",
      "hidden_size:  128 \n",
      "num_layers:  3 \n",
      "bias =  False \n",
      "batch_first =  True \n",
      "dropout =  0.0 \n",
      "bidirectional =  True \n",
      "num_epochs =  11 \n",
      "learning_rate =  0.01 \n",
      "batch_size =  16 \n",
      "=========================\n",
      "epoch: 0\n",
      "train loss: 0.684606  [   16/141120]\n",
      "train loss: 0.467334  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.594305 \n",
      "\n",
      "epoch: 1\n",
      "train loss: 0.418013  [   16/141120]\n",
      "train loss: 0.490578  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.594917 \n",
      "\n",
      "epoch: 2\n",
      "train loss: 0.472698  [   16/141120]\n",
      "train loss: 0.549325  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.592564 \n",
      "\n",
      "epoch: 3\n",
      "train loss: 0.480576  [   16/141120]\n",
      "train loss: 0.677660  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.593747 \n",
      "\n",
      "epoch: 4\n",
      "train loss: 0.580181  [   16/141120]\n",
      "train loss: 0.727279  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.588857 \n",
      "\n",
      "epoch: 5\n",
      "train loss: 0.735742  [   16/141120]\n",
      "train loss: 0.610138  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.573956 \n",
      "\n",
      "epoch: 6\n",
      "train loss: 0.529516  [   16/141120]\n",
      "train loss: 0.421628  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 71.3%, Avg loss: 0.566327 \n",
      "\n",
      "epoch: 7\n",
      "train loss: 0.415201  [   16/141120]\n",
      "train loss: 0.558775  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 71.8%, Avg loss: 0.553679 \n",
      "\n",
      "epoch: 8\n",
      "train loss: 0.535431  [   16/141120]\n",
      "train loss: 0.692262  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 0.456718 \n",
      "\n",
      "epoch: 9\n",
      "train loss: 0.483682  [   16/141120]\n",
      "train loss: 0.379410  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 76.4%, Avg loss: 0.394376 \n",
      "\n",
      "epoch: 10\n",
      "train loss: 0.371960  [   16/141120]\n",
      "train loss: 0.344067  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 0.378568 \n",
      "\n",
      "=========================\n",
      "Trials from Best to Worst\n",
      "=========================\n",
      "\n",
      "1: Trial #6, Accuracy = 84.64569160997732, Avg Test Loss = 0.2935680150985718, Hidden Size = 64, Num Layers = 5, Bias = False, Batch First = False, Dropout = 0.0, Bidirectional = False, Num Epochs = 14, Learning Rate = 0.01, Batch Size = 16\n",
      "2: Trial #1, Accuracy = 81.7063492063492, Avg Test Loss = 0.33433154225349426, Hidden Size = 64, Num Layers = 2, Bias = False, Batch First = True, Dropout = 0.0, Bidirectional = True, Num Epochs = 13, Learning Rate = 0.01, Batch Size = 16\n",
      "3: Trial #11, Accuracy = 79.77607709750568, Avg Test Loss = 0.3563286066055298, Hidden Size = 64, Num Layers = 2, Bias = False, Batch First = True, Dropout = 0.0, Bidirectional = False, Num Epochs = 10, Learning Rate = 0.01, Batch Size = 16\n",
      "4: Trial #4, Accuracy = 78.44104308390023, Avg Test Loss = 0.3556157946586609, Hidden Size = 32, Num Layers = 4, Bias = False, Batch First = False, Dropout = 0.0, Bidirectional = True, Num Epochs = 15, Learning Rate = 0.01, Batch Size = 16\n",
      "5: Trial #5, Accuracy = 77.5907029478458, Avg Test Loss = 0.37634673714637756, Hidden Size = 32, Num Layers = 2, Bias = True, Batch First = False, Dropout = 0.0, Bidirectional = False, Num Epochs = 10, Learning Rate = 0.01, Batch Size = 16\n",
      "6: Trial #12, Accuracy = 77.49716553287982, Avg Test Loss = 0.37452423572540283, Hidden Size = 64, Num Layers = 3, Bias = False, Batch First = True, Dropout = 0.0, Bidirectional = True, Num Epochs = 11, Learning Rate = 0.01, Batch Size = 16\n",
      "7: Trial #14, Accuracy = 77.28458049886622, Avg Test Loss = 0.3785683810710907, Hidden Size = 128, Num Layers = 3, Bias = False, Batch First = True, Dropout = 0.0, Bidirectional = True, Num Epochs = 11, Learning Rate = 0.01, Batch Size = 16\n",
      "8: Trial #10, Accuracy = 71.57879818594104, Avg Test Loss = 0.5488039255142212, Hidden Size = 256, Num Layers = 3, Bias = True, Batch First = True, Dropout = 0.0, Bidirectional = True, Num Epochs = 10, Learning Rate = 0.01, Batch Size = 16\n",
      "9: Trial #2, Accuracy = 70.94954648526077, Avg Test Loss = 0.5936791300773621, Hidden Size = 64, Num Layers = 4, Bias = False, Batch First = True, Dropout = 0.0, Bidirectional = True, Num Epochs = 10, Learning Rate = 0.001, Batch Size = 16\n",
      "10: Trial #13, Accuracy = 70.9297052154195, Avg Test Loss = 0.5943533182144165, Hidden Size = 32, Num Layers = 4, Bias = True, Batch First = True, Dropout = 0.0, Bidirectional = True, Num Epochs = 10, Learning Rate = 0.001, Batch Size = 16\n",
      "11: Trial #7, Accuracy = 70.66043083900226, Avg Test Loss = 0.5951948761940002, Hidden Size = 64, Num Layers = 2, Bias = False, Batch First = False, Dropout = 0.0, Bidirectional = True, Num Epochs = 11, Learning Rate = 0.001, Batch Size = 16\n",
      "12: Trial #3, Accuracy = 70.65192743764173, Avg Test Loss = 0.5930311679840088, Hidden Size = 32, Num Layers = 3, Bias = False, Batch First = False, Dropout = 0.0, Bidirectional = False, Num Epochs = 11, Learning Rate = 0.001, Batch Size = 16\n",
      "13: Trial #8, Accuracy = 70.65192743764173, Avg Test Loss = 0.5923343300819397, Hidden Size = 128, Num Layers = 4, Bias = True, Batch First = False, Dropout = 0.0, Bidirectional = False, Num Epochs = 14, Learning Rate = 0.001, Batch Size = 16\n",
      "14: Trial #9, Accuracy = 70.65192743764173, Avg Test Loss = 0.5922024846076965, Hidden Size = 256, Num Layers = 4, Bias = False, Batch First = True, Dropout = 0.0, Bidirectional = False, Num Epochs = 13, Learning Rate = 0.001, Batch Size = 16\n",
      "============================================================================================== \n",
      "Trial 15 \n",
      "\n",
      "hidden_size:  256 \n",
      "num_layers:  2 \n",
      "bias =  True \n",
      "batch_first =  False \n",
      "dropout =  0.0 \n",
      "bidirectional =  False \n",
      "num_epochs =  12 \n",
      "learning_rate =  0.01 \n",
      "batch_size =  16 \n",
      "=========================\n",
      "epoch: 0\n",
      "train loss: 0.711404  [   16/141120]\n",
      "train loss: 0.583611  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.595116 \n",
      "\n",
      "epoch: 1\n",
      "train loss: 0.558941  [   16/141120]\n",
      "train loss: 0.564855  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.595857 \n",
      "\n",
      "epoch: 2\n",
      "train loss: 0.571620  [   16/141120]\n",
      "train loss: 0.487820  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.594693 \n",
      "\n",
      "epoch: 3\n",
      "train loss: 0.682655  [   16/141120]\n",
      "train loss: 0.666241  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.592962 \n",
      "\n",
      "epoch: 4\n",
      "train loss: 0.761405  [   16/141120]\n",
      "train loss: 0.598033  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.591194 \n",
      "\n",
      "epoch: 5\n",
      "train loss: 0.605727  [   16/141120]\n",
      "train loss: 0.637245  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.590518 \n",
      "\n",
      "epoch: 6\n",
      "train loss: 0.588598  [   16/141120]\n",
      "train loss: 0.504960  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.590435 \n",
      "\n",
      "epoch: 7\n",
      "train loss: 0.516467  [   16/141120]\n",
      "train loss: 0.581727  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 71.2%, Avg loss: 0.580643 \n",
      "\n",
      "epoch: 8\n",
      "train loss: 0.579457  [   16/141120]\n",
      "train loss: 0.760408  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 0.574384 \n",
      "\n",
      "epoch: 9\n",
      "train loss: 0.637805  [   16/141120]\n",
      "train loss: 0.549968  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 71.2%, Avg loss: 0.574837 \n",
      "\n",
      "epoch: 10\n",
      "train loss: 0.790845  [   16/141120]\n",
      "train loss: 0.691355  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 71.2%, Avg loss: 0.572869 \n",
      "\n",
      "epoch: 11\n",
      "train loss: 0.610580  [   16/141120]\n",
      "train loss: 0.686342  [80016/141120]\n",
      "Test Error: \n",
      " Accuracy: 71.2%, Avg loss: 0.570748 \n",
      "\n",
      "=========================\n",
      "Trials from Best to Worst\n",
      "=========================\n",
      "\n",
      "1: Trial #6, Accuracy = 84.64569160997732, Avg Test Loss = 0.2935680150985718, Hidden Size = 64, Num Layers = 5, Bias = False, Batch First = False, Dropout = 0.0, Bidirectional = False, Num Epochs = 14, Learning Rate = 0.01, Batch Size = 16\n",
      "2: Trial #1, Accuracy = 81.7063492063492, Avg Test Loss = 0.33433154225349426, Hidden Size = 64, Num Layers = 2, Bias = False, Batch First = True, Dropout = 0.0, Bidirectional = True, Num Epochs = 13, Learning Rate = 0.01, Batch Size = 16\n",
      "3: Trial #11, Accuracy = 79.77607709750568, Avg Test Loss = 0.3563286066055298, Hidden Size = 64, Num Layers = 2, Bias = False, Batch First = True, Dropout = 0.0, Bidirectional = False, Num Epochs = 10, Learning Rate = 0.01, Batch Size = 16\n",
      "4: Trial #4, Accuracy = 78.44104308390023, Avg Test Loss = 0.3556157946586609, Hidden Size = 32, Num Layers = 4, Bias = False, Batch First = False, Dropout = 0.0, Bidirectional = True, Num Epochs = 15, Learning Rate = 0.01, Batch Size = 16\n",
      "5: Trial #5, Accuracy = 77.5907029478458, Avg Test Loss = 0.37634673714637756, Hidden Size = 32, Num Layers = 2, Bias = True, Batch First = False, Dropout = 0.0, Bidirectional = False, Num Epochs = 10, Learning Rate = 0.01, Batch Size = 16\n",
      "6: Trial #12, Accuracy = 77.49716553287982, Avg Test Loss = 0.37452423572540283, Hidden Size = 64, Num Layers = 3, Bias = False, Batch First = True, Dropout = 0.0, Bidirectional = True, Num Epochs = 11, Learning Rate = 0.01, Batch Size = 16\n",
      "7: Trial #14, Accuracy = 77.28458049886622, Avg Test Loss = 0.3785683810710907, Hidden Size = 128, Num Layers = 3, Bias = False, Batch First = True, Dropout = 0.0, Bidirectional = True, Num Epochs = 11, Learning Rate = 0.01, Batch Size = 16\n",
      "8: Trial #10, Accuracy = 71.57879818594104, Avg Test Loss = 0.5488039255142212, Hidden Size = 256, Num Layers = 3, Bias = True, Batch First = True, Dropout = 0.0, Bidirectional = True, Num Epochs = 10, Learning Rate = 0.01, Batch Size = 16\n",
      "9: Trial #15, Accuracy = 71.21315192743765, Avg Test Loss = 0.5707476139068604, Hidden Size = 256, Num Layers = 2, Bias = True, Batch First = False, Dropout = 0.0, Bidirectional = False, Num Epochs = 12, Learning Rate = 0.01, Batch Size = 16\n",
      "10: Trial #2, Accuracy = 70.94954648526077, Avg Test Loss = 0.5936791300773621, Hidden Size = 64, Num Layers = 4, Bias = False, Batch First = True, Dropout = 0.0, Bidirectional = True, Num Epochs = 10, Learning Rate = 0.001, Batch Size = 16\n",
      "11: Trial #13, Accuracy = 70.9297052154195, Avg Test Loss = 0.5943533182144165, Hidden Size = 32, Num Layers = 4, Bias = True, Batch First = True, Dropout = 0.0, Bidirectional = True, Num Epochs = 10, Learning Rate = 0.001, Batch Size = 16\n",
      "12: Trial #7, Accuracy = 70.66043083900226, Avg Test Loss = 0.5951948761940002, Hidden Size = 64, Num Layers = 2, Bias = False, Batch First = False, Dropout = 0.0, Bidirectional = True, Num Epochs = 11, Learning Rate = 0.001, Batch Size = 16\n",
      "13: Trial #3, Accuracy = 70.65192743764173, Avg Test Loss = 0.5930311679840088, Hidden Size = 32, Num Layers = 3, Bias = False, Batch First = False, Dropout = 0.0, Bidirectional = False, Num Epochs = 11, Learning Rate = 0.001, Batch Size = 16\n",
      "14: Trial #8, Accuracy = 70.65192743764173, Avg Test Loss = 0.5923343300819397, Hidden Size = 128, Num Layers = 4, Bias = True, Batch First = False, Dropout = 0.0, Bidirectional = False, Num Epochs = 14, Learning Rate = 0.001, Batch Size = 16\n",
      "15: Trial #9, Accuracy = 70.65192743764173, Avg Test Loss = 0.5922024846076965, Hidden Size = 256, Num Layers = 4, Bias = False, Batch First = True, Dropout = 0.0, Bidirectional = False, Num Epochs = 13, Learning Rate = 0.001, Batch Size = 16\n"
     ]
    }
   ],
   "source": [
    "num_trials = 15\n",
    "for trial in range(num_trials):\n",
    "    best_accuracy = 0\n",
    "    best_params = None\n",
    "\n",
    "    input_size = embed_dim\n",
    "    hidden_size = random.choice([32, 64, 128, 256])\n",
    "    num_layers = random.choice([2,3,4,5])\n",
    "    bias = random.choice([True, False])\n",
    "    batch_first = random.choice([True, False])\n",
    "    dropout = 0.0    \n",
    "    bidirectional = random.choice([True, False])\n",
    "    output_size = 2\n",
    "    num_epochs = random.choice([10, 11, 12, 13, 14, 15])\n",
    "    #num_epochs = random.choice([1])\n",
    "    learning_rate = random.choice([0.01, 0.001]) # If you set this too high, it might explode. If too low, it might not learn\n",
    "\n",
    "    #Loading in data!\n",
    "    dataloader = DataLoader(input_dataset, shuffle=True, batch_size=batch_size, num_workers=0, collate_fn=my_collate_fn)\n",
    "    test_dataloader = DataLoader(test_dataset, shuffle=True, batch_size=batch_size, num_workers=0, collate_fn=my_collate_fn)\n",
    "\n",
    "    rnn = GRU(input_size, hidden_size, num_layers, bias, batch_first, dropout, bidirectional, output_size)\n",
    "    rnn.to(device)\n",
    "\n",
    "    optimizer = torch.optim.SGD(rnn.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(\"==============================================================================================\",\n",
    "        \"\\nTrial\", str(trial + 1), \"\\n\"\n",
    "        #\"\\noptimizer: \", optimizer, \"\\ncriterion: \", criterion,\n",
    "        \"\\nhidden_size: \", hidden_size, \"\\nnum_layers: \", num_layers, \n",
    "        \"\\nbias = \", bias, \"\\nbatch_first = \", batch_first, \n",
    "        \"\\ndropout = \", dropout, \"\\nbidirectional = \", bidirectional, \n",
    "        \"\\nnum_epochs = \", num_epochs, \"\\nlearning_rate = \", learning_rate, \n",
    "        \"\\nbatch_size = \", batch_size,\n",
    "        \"\\n=========================\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"epoch: \"+str(epoch))\n",
    "        \n",
    "        size = len(dataloader.dataset)\n",
    "        rnn.train()\n",
    "        for batch,(x_padded, y_padded, x_lens, y_lens) in enumerate(dataloader):\n",
    "\n",
    "            \n",
    "            rnn.zero_grad()\n",
    "            x_embed = embed(x_padded)\n",
    "            x_packed = pack_padded_sequence(x_embed, x_lens, batch_first=False, enforce_sorted=False)\n",
    "            x_packed = x_packed.to(device)\n",
    "            y_padded = y_padded.to(device)\n",
    "            output, hidden = rnn(x_packed)\n",
    "            output = torch.reshape(output, (batch_size,2))\n",
    "            \n",
    "            y_padded = torch.reshape(y_padded, (batch_size,))\n",
    "            y_padded = y_padded.long()\n",
    "            loss = criterion(output, y_padded)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "\n",
    "            if batch % 5000 == 0:\n",
    "                x_s = np.shape(x_padded)[1]\n",
    "                loss, current = loss.item(), (batch + 1) * x_s\n",
    "                print(f\"train loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "\n",
    "        # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "        # Unnecessary in this situation but added for best practices\n",
    "        rnn.eval()\n",
    "        size = len(test_dataloader.dataset)\n",
    "        num_batches = len(test_dataloader)\n",
    "        test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "        # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "        with torch.no_grad():\n",
    "            for batch,(x_padded, y_padded, x_lens, y_lens) in enumerate(test_dataloader):\n",
    "                rnn.zero_grad()\n",
    "                x_embed = embed(x_padded)\n",
    "                x_packed = pack_padded_sequence(x_embed, x_lens, batch_first=False, enforce_sorted=False)\n",
    "                x_packed = x_packed.to(device)\n",
    "                y_padded = y_padded.to(device)\n",
    "                output, hidden = rnn(x_packed)\n",
    "                output = torch.reshape(output, (batch_size,2))\n",
    "                \n",
    "                y_padded = torch.reshape(y_padded, (batch_size,))\n",
    "                y_padded = y_padded.long()\n",
    "                correct += (output.argmax(1) == y_padded).type(torch.float).sum().item()\n",
    "                loss += criterion(output, y_padded)\n",
    "            \n",
    "            #loss, current = loss, (batch + 1) * len(x_padded)\n",
    "            test_loss = loss/num_batches\n",
    "            correct /= size\n",
    "            accuracy = 100*correct\n",
    "            print(f\"Test Error: \\n Accuracy: {accuracy:>0.1f}%, Avg loss: {test_loss:>8f} \\n\") \n",
    "    hyperparams = {\n",
    "        'trial': (trial+1),\n",
    "        'accuracy': accuracy,\n",
    "        'avg_test_loss': test_loss,\n",
    "        'hidden_size': hidden_size,\n",
    "        'num_layers': num_layers,\n",
    "        'bias': bias,\n",
    "        'batch_first': batch_first,\n",
    "        'dropout': dropout, \n",
    "        'bidirectional': bidirectional,\n",
    "        'num_epochs': num_epochs,\n",
    "        'learning_rate': learning_rate, \n",
    "        'batch_size': batch_size\n",
    "    }\n",
    "    add_to_list_of_params(hyperparams)\n",
    "    print_all_trials()\n",
    "\n",
    "    '''\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_params = {\n",
    "            'hidden_size': hidden_size,\n",
    "            'num_layers': num_layers,\n",
    "            'bias': bias,\n",
    "            'batch_first': batch_first,\n",
    "            'dropout': dropout, \n",
    "            'bidirectional': bidirectional,\n",
    "            'num_epochs': num_epochs,\n",
    "            'learning_rate': learning_rate, \n",
    "            'batch_size': batch_size\n",
    "        }\n",
    "    '''\n",
    "\n",
    "#print(f\"Best Accuracy: {best_accuracy:.2f}%\")\n",
    "#print(f\"Best Parameters: {best_params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training (Original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnum_epochs = 13\\nlearning_rate = 0.01 # If you set this too high, it might explode. If too low, it might not learn\\noptimizer = torch.optim.SGD(rnn.parameters(), lr=learning_rate)\\n\\ncriterion = nn.CrossEntropyLoss()\\n\\nprint(\"===============================================\",\\n      #\"\\noptimizer: \", optimizer, \"\\ncriterion: \", criterion,\\n      \"\\nhidden_size: \", hidden_size, \"\\nnum_layers: \", num_layers, \\n      \"\\nbias = \", bias, \"\\nbatch_first = \", batch_first, \\n      \"\\ndropout = \", dropout, \"\\nbidirectional = \", bidirectional, \\n      \"\\nnum_epochs = \", num_epochs, \"\\nlearning_rate = \", learning_rate, \\n      \"\\nbatch_size = \", batch_size,\\n      \"\\n===============================================\")\\n\\nfor epoch in range(num_epochs):\\n    print(\"epoch: \"+str(epoch))\\n    size = len(dataloader.dataset)\\n    rnn.train()\\n    for batch,(x_padded, y_padded, x_lens, y_lens) in enumerate(dataloader):\\n\\n        \\n        rnn.zero_grad()\\n        x_embed = embed(x_padded)\\n        x_packed = pack_padded_sequence(x_embed, x_lens, batch_first=False, enforce_sorted=False)\\n        x_packed = x_packed.to(device)\\n        y_padded = y_padded.to(device)\\n        output, hidden = rnn(x_packed)\\n        output = torch.reshape(output, (batch_size,2))\\n        \\n        y_padded = torch.reshape(y_padded, (batch_size,))\\n        y_padded = y_padded.long()\\n        loss = criterion(output, y_padded)\\n\\n        loss.backward()\\n        optimizer.step()\\n        \\n\\n        if batch % 5000 == 0:\\n            x_s = np.shape(x_padded)[1]\\n            loss, current = loss.item(), (batch + 1) * x_s\\n            print(f\"train loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\\n\\n\\n\\n    # Set the model to evaluation mode - important for batch normalization and dropout layers\\n    # Unnecessary in this situation but added for best practices\\n    rnn.eval()\\n    size = len(test_dataloader.dataset)\\n    num_batches = len(test_dataloader)\\n    test_loss, correct = 0, 0\\n\\n # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\\n    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\\n    with torch.no_grad():\\n        for batch,(x_padded, y_padded, x_lens, y_lens) in enumerate(test_dataloader):\\n            rnn.zero_grad()\\n            x_embed = embed(x_padded)\\n            x_packed = pack_padded_sequence(x_embed, x_lens, batch_first=False, enforce_sorted=False)\\n            x_packed = x_packed.to(device)\\n            y_padded = y_padded.to(device)\\n            output, hidden = rnn(x_packed)\\n            output = torch.reshape(output, (batch_size,2))\\n            \\n            y_padded = torch.reshape(y_padded, (batch_size,))\\n            y_padded = y_padded.long()\\n            correct += (output.argmax(1) == y_padded).type(torch.float).sum().item()\\n            loss += criterion(output, y_padded)\\n        \\n        #loss, current = loss, (batch + 1) * len(x_padded)\\n        test_loss = loss/num_batches\\n        correct /= size\\n        print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\") \\n\\n'"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "num_epochs = 13\n",
    "learning_rate = 0.01 # If you set this too high, it might explode. If too low, it might not learn\n",
    "optimizer = torch.optim.SGD(rnn.parameters(), lr=learning_rate)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"===============================================\",\n",
    "      #\"\\noptimizer: \", optimizer, \"\\ncriterion: \", criterion,\n",
    "      \"\\nhidden_size: \", hidden_size, \"\\nnum_layers: \", num_layers, \n",
    "      \"\\nbias = \", bias, \"\\nbatch_first = \", batch_first, \n",
    "      \"\\ndropout = \", dropout, \"\\nbidirectional = \", bidirectional, \n",
    "      \"\\nnum_epochs = \", num_epochs, \"\\nlearning_rate = \", learning_rate, \n",
    "      \"\\nbatch_size = \", batch_size,\n",
    "      \"\\n===============================================\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"epoch: \"+str(epoch))\n",
    "    size = len(dataloader.dataset)\n",
    "    rnn.train()\n",
    "    for batch,(x_padded, y_padded, x_lens, y_lens) in enumerate(dataloader):\n",
    "\n",
    "        \n",
    "        rnn.zero_grad()\n",
    "        x_embed = embed(x_padded)\n",
    "        x_packed = pack_padded_sequence(x_embed, x_lens, batch_first=False, enforce_sorted=False)\n",
    "        x_packed = x_packed.to(device)\n",
    "        y_padded = y_padded.to(device)\n",
    "        output, hidden = rnn(x_packed)\n",
    "        output = torch.reshape(output, (batch_size,2))\n",
    "        \n",
    "        y_padded = torch.reshape(y_padded, (batch_size,))\n",
    "        y_padded = y_padded.long()\n",
    "        loss = criterion(output, y_padded)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        if batch % 5000 == 0:\n",
    "            x_s = np.shape(x_padded)[1]\n",
    "            loss, current = loss.item(), (batch + 1) * x_s\n",
    "            print(f\"train loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    rnn.eval()\n",
    "    size = len(test_dataloader.dataset)\n",
    "    num_batches = len(test_dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    " # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for batch,(x_padded, y_padded, x_lens, y_lens) in enumerate(test_dataloader):\n",
    "            rnn.zero_grad()\n",
    "            x_embed = embed(x_padded)\n",
    "            x_packed = pack_padded_sequence(x_embed, x_lens, batch_first=False, enforce_sorted=False)\n",
    "            x_packed = x_packed.to(device)\n",
    "            y_padded = y_padded.to(device)\n",
    "            output, hidden = rnn(x_packed)\n",
    "            output = torch.reshape(output, (batch_size,2))\n",
    "            \n",
    "            y_padded = torch.reshape(y_padded, (batch_size,))\n",
    "            y_padded = y_padded.long()\n",
    "            correct += (output.argmax(1) == y_padded).type(torch.float).sum().item()\n",
    "            loss += criterion(output, y_padded)\n",
    "        \n",
    "        #loss, current = loss, (batch + 1) * len(x_padded)\n",
    "        test_loss = loss/num_batches\n",
    "        correct /= size\n",
    "        print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\") \n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
