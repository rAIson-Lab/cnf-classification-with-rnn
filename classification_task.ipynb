{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import Embedding, RNN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set any global values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f5857a74190>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('theorum.pickle','rb') as f:\n",
    "    theorum_data = pickle.load(f)\n",
    "\n",
    "with open('non_theorum.pickle','rb') as f:\n",
    "    non_theorum_data = pickle.load(f)\n",
    "\n",
    "a = theorum_data\n",
    "b = non_theorum_data\n",
    "\n",
    "training_data = []\n",
    "xtrain = []\n",
    "ytrain = []\n",
    "\n",
    "\n",
    "random.shuffle(a)\n",
    "random.shuffle(b)\n",
    "\n",
    "if len(a) < len(b):\n",
    "    total = len(a)\n",
    "else:\n",
    "    total = len(b)\n",
    "\n",
    "for i in range(total):\n",
    "    training_data.append(a[i])\n",
    "    training_data.append(b[i])\n",
    "    xtrain.append(a[i][0])\n",
    "    ytrain.append(a[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104736\n"
     ]
    }
   ],
   "source": [
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('theorum_v2.pickle','rb') as f:\n",
    "    theorum_data2 = pickle.load(f)\n",
    "\n",
    "with open('non_theorum_v2.pickle','rb') as f:\n",
    "    non_theorum_data2 = pickle.load(f)\n",
    "\n",
    "a = theorum_data2\n",
    "b = non_theorum_data2\n",
    "\n",
    "\n",
    "random.shuffle(a)\n",
    "random.shuffle(b)\n",
    "xtest = []\n",
    "ytest = []\n",
    "\n",
    "if len(a) < len(b):\n",
    "    total = len(a)\n",
    "else:\n",
    "    total = len(b)\n",
    "for i in range(total):\n",
    "    training_data.append(a[i])\n",
    "    training_data.append(b[i])\n",
    "    xtest.append(b[i][0])\n",
    "    ytest.append(b[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(~b|~a)&(~b).(~a|b)&(b|b).>(~b|a)&(~a|~b).', 'Found']\n",
      "['(a|b)&(a|b).(~a)&(b|~b).>(~b|~b)&(~a|~a).', 'Unfound']\n"
     ]
    }
   ],
   "source": [
    "print(theorum_data2[-1])\n",
    "print(non_theorum_data2[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.150551872533562 Found\n",
      "31.390997552072896 Unfound\n"
     ]
    }
   ],
   "source": [
    "theorum_length = 0\n",
    "theorum_count = 0\n",
    "non_theorum_length = 0\n",
    "non_theurum_count = 0\n",
    "\n",
    "for example in training_data:\n",
    "    if example[1] == 'Found':\n",
    "        theorum_length += len(example[0])-2\n",
    "        #print(len(example[0])-2)\n",
    "        theorum_count += 1\n",
    "    if example[1] == 'Unfound':\n",
    "        non_theorum_length += len(example[0])-2\n",
    "        #print(len(example[0])-2)\n",
    "        non_theurum_count += 1\n",
    "    \n",
    "print(theorum_length/theorum_count, \"Found\")\n",
    "print(non_theorum_length/non_theurum_count, \"Unfound\")\n",
    "#print(theorum_count)\n",
    "#print(non_theurum_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_to_ix = {\"Found\":0, \"Unfound\":1}\n",
    "all_letters = \"()ab~&|>.\"\n",
    "\n",
    "vocab = ['<pad>'] + sorted(set([char for seq in all_letters for char in seq]))\n",
    "\n",
    "\n",
    "n_classes = len(class_to_ix)\n",
    "n_letters = len(all_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "embed_dim = len(vocab)\n",
    "embed = Embedding(len(vocab), embed_dim) # embedding_dim = len(vocab)\n",
    "for element in training_data:\n",
    "    input = [vocab.index(token) for token in element[0]]\n",
    "    input_tensor = torch.tensor(input, dtype=torch.int)\n",
    "    classification = class_to_ix[element[1]]\n",
    "    X.append(input_tensor)\n",
    "    output_tensor = torch.tensor(classification, dtype=torch.int)\n",
    "    Y.append(output_tensor)\n",
    "\n",
    "new_xtrain = []\n",
    "new_ytrain = []\n",
    "new_xtest = []\n",
    "new_ytest = []\n",
    "for element in xtrain:\n",
    "    input = [vocab.index(token) for token in element]\n",
    "    input_tensor = torch.tensor(input, dtype=torch.int)\n",
    "    new_xtrain.append(input_tensor)\n",
    "for element in ytrain:\n",
    "    classification = class_to_ix[element]\n",
    "    output_tensor = torch.tensor(classification, dtype=torch.int)\n",
    "    new_ytrain.append(output_tensor)\n",
    "\n",
    "for element in xtest:\n",
    "    input = [vocab.index(token) for token in element]\n",
    "    input_tensor = torch.tensor(input, dtype=torch.int)\n",
    "    new_xtest.append(input_tensor)\n",
    "for element in ytest:\n",
    "    classification = class_to_ix[element]\n",
    "    output_tensor = torch.tensor(classification, dtype=torch.int)\n",
    "    new_ytest.append(output_tensor)\n",
    "\n",
    "\n",
    "xtrain = new_xtrain\n",
    "ytrain = new_ytrain\n",
    "xtest = new_xtest\n",
    "ytest = new_ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TTPStyleDataset(Dataset):\n",
    "    def __init__(self, X,Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        sample = [self.X[idx], self.Y[idx]]\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "input_dataset = TTPStyleDataset(X_train,y_train)\n",
    "\n",
    "test_dataset = TTPStyleDataset(X_test,y_test)\n",
    "\n",
    "#uncomment the following to split such that you train on the small dataset, and test on the longer one.\n",
    "#print(len(xtrain))\n",
    "#print(len(ytrain))\n",
    "#print(len(xtest))\n",
    "#print(len(ytest))\n",
    "#xtrain, _, ytrain, _ = train_test_split(xtrain, ytrain, test_size=0.001, random_state=42)\n",
    "#_, xtest, _, ytest = train_test_split(xtest, ytest, test_size=.999, random_state=42)\n",
    "#print(len(xtrain))\n",
    "#print(len(ytrain))\n",
    "#print(len(xtest))\n",
    "#print(len(ytest))\n",
    "\n",
    "\n",
    "\n",
    "#input_dataset = TTPStyleDataset(xtrain,ytrain)\n",
    "\n",
    "#test_dataset = TTPStyleDataset(xtest,ytest)\n",
    "\n",
    "def my_collate_fn(data):\n",
    "    (xx,yy) = zip(*data)\n",
    "    x_lens = [len(x) for x in xx]\n",
    "    y_lens = [1 for y in yy]\n",
    "\n",
    "    xx_pad = pad_sequence(xx, batch_first=False, padding_value=0)\n",
    "    yy_pad = torch.tensor(yy)\n",
    "\n",
    "    return xx_pad, yy_pad, x_lens, y_lens\n",
    "    \n",
    "\n",
    "dataloader = DataLoader(input_dataset, shuffle=True, batch_size=batch_size, num_workers=0, collate_fn=my_collate_fn, drop_last=True)\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=True, batch_size=batch_size, num_workers=0, collate_fn=my_collate_fn, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxRNN(nn.RNN):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, nonlinearity, bias, batch_first, dropout, bidirectional, output_size):\n",
    "        super(SoftmaxRNN, self).__init__(input_size, hidden_size, num_layers, nonlinearity, bias, batch_first, dropout, bidirectional)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    \n",
    "    def forward(self, x, h_0=None):\n",
    "        output, h = super(SoftmaxRNN, self).forward(x, h_0)\n",
    "        output, seq_len = pad_packed_sequence(output, batch_first=False)\n",
    "        output = self.fc(h[-1])\n",
    "        return output, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SoftmaxRNN(\n",
       "  10, 32\n",
       "  (fc): Linear(in_features=32, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#input_size - The number of expected features in the input x\n",
    "rnn = SoftmaxRNN(input_size=embed_dim, hidden_size=32, num_layers=1, nonlinearity='relu', bias=True, batch_first=False, dropout=0.0, bidirectional=False, output_size=2)\n",
    "rnn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "Test Error: Accuracy: 50.0%, Avg loss: 0.692629\n",
      "epoch: 1\n",
      "Test Error: Accuracy: 57.9%, Avg loss: 0.678674\n",
      "epoch: 2\n",
      "Test Error: Accuracy: 59.2%, Avg loss: 0.674545\n",
      "epoch: 3\n",
      "Test Error: Accuracy: 70.6%, Avg loss: 0.563428\n",
      "epoch: 4\n",
      "Test Error: Accuracy: 73.0%, Avg loss: 0.524852\n",
      "epoch: 5\n",
      "Test Error: Accuracy: 81.0%, Avg loss: 0.408008\n",
      "epoch: 6\n",
      "Test Error: Accuracy: 80.3%, Avg loss: 0.466041\n",
      "epoch: 7\n",
      "Test Error: Accuracy: 81.0%, Avg loss: 0.389833\n",
      "epoch: 8\n",
      "Test Error: Accuracy: 81.6%, Avg loss: 0.368423\n",
      "epoch: 9\n",
      "Test Error: Accuracy: 82.3%, Avg loss: 0.360505\n",
      "epoch: 10\n",
      "Test Error: Accuracy: 82.0%, Avg loss: 0.361275\n",
      "epoch: 11\n",
      "Test Error: Accuracy: 81.5%, Avg loss: 0.371570\n",
      "epoch: 12\n",
      "Test Error: Accuracy: 81.1%, Avg loss: 0.381624\n",
      "epoch: 13\n",
      "Test Error: Accuracy: 80.6%, Avg loss: 0.413711\n",
      "epoch: 14\n",
      "Test Error: Accuracy: 81.4%, Avg loss: 0.374862\n",
      "epoch: 15\n",
      "Test Error: Accuracy: 82.9%, Avg loss: 0.341451\n",
      "epoch: 16\n",
      "Test Error: Accuracy: 83.4%, Avg loss: 0.329774\n",
      "epoch: 17\n",
      "Test Error: Accuracy: 84.0%, Avg loss: 0.317540\n",
      "epoch: 18\n",
      "Test Error: Accuracy: 63.1%, Avg loss: 0.631322\n",
      "epoch: 19\n",
      "Test Error: Accuracy: 70.1%, Avg loss: 0.562377\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "learning_rate = 0.01 # If you set this too high, it might explode. If too low, it might not learn\n",
    "optimizer = torch.optim.SGD(rnn.parameters(), lr=learning_rate)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"epoch: \"+str(epoch))\n",
    "    size = len(dataloader.dataset)\n",
    "    rnn.train()\n",
    "    for batch,(x_padded, y_padded, x_lens, y_lens) in enumerate(dataloader):\n",
    "\n",
    "        \n",
    "        rnn.zero_grad()\n",
    "        x_embed = embed(x_padded)\n",
    "        x_packed = pack_padded_sequence(x_embed, x_lens, batch_first=False, enforce_sorted=False)\n",
    "        x_packed = x_packed.to(device)\n",
    "        y_padded = y_padded.to(device)\n",
    "        output, hidden = rnn(x_packed)\n",
    "        output = torch.reshape(output, (batch_size,2))\n",
    "\n",
    "        \n",
    "        y_padded = torch.reshape(y_padded, (batch_size,))\n",
    "        y_padded = y_padded.long()\n",
    "        loss = criterion(output, y_padded)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        if batch % (64*1000) == 0:\n",
    "            x_s = np.shape(x_padded)[1]\n",
    "            loss, current = loss.item(), (batch + 1) * x_s\n",
    "            #print(f\"train loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    rnn.eval()\n",
    "    size = len(test_dataloader.dataset)\n",
    "    num_batches = len(test_dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for batch,(x_padded, y_padded, x_lens, y_lens) in enumerate(test_dataloader):\n",
    "            rnn.zero_grad()\n",
    "            x_embed = embed(x_padded)\n",
    "            x_packed = pack_padded_sequence(x_embed, x_lens, batch_first=False, enforce_sorted=False)\n",
    "            x_packed = x_packed.to(device)\n",
    "            y_padded = y_padded.to(device)\n",
    "            output, hidden = rnn(x_packed)\n",
    "            output = torch.reshape(output, (batch_size,2))\n",
    "            \n",
    "            y_padded = torch.reshape(y_padded, (batch_size,))\n",
    "            y_padded = y_padded.long()\n",
    "            correct += (output.argmax(1) == y_padded).type(torch.float).sum().item()\n",
    "            loss += criterion(output, y_padded)\n",
    "        \n",
    "        #loss, current = loss, (batch + 1) * len(x_padded)\n",
    "        test_loss = loss/num_batches\n",
    "        correct /= size\n",
    "        print(f\"Test Error: Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1989696\n"
     ]
    }
   ],
   "source": [
    "print(size*batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
