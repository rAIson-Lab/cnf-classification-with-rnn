{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import Embedding, RNN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set any global values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f572a5c7810>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('training_data.pickle','rb') as f:\n",
    "    training_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_to_ix = {\"Found\":0, \"Unfound\":1}\n",
    "all_letters = \"()ab~&|>\"\n",
    "\n",
    "vocab = ['<pad>'] + sorted(set([char for seq in all_letters for char in seq]))\n",
    "\n",
    "\n",
    "n_classes = len(class_to_ix)\n",
    "n_letters = len(all_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "embed_dim = len(vocab)\n",
    "embed = Embedding(len(vocab), embed_dim) # embedding_dim = len(vocab)\n",
    "for element in training_data:\n",
    "    input = [vocab.index(token) for token in element[0]]\n",
    "    input_tensor = torch.tensor(input, dtype=torch.int)\n",
    "    classification = class_to_ix[element[1]]\n",
    "    X.append(input_tensor)\n",
    "    output_tensor = torch.tensor(classification, dtype=torch.int)\n",
    "    Y.append(output_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TTPStyleDataset(Dataset):\n",
    "    def __init__(self, X,Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        sample = [self.X[idx], self.Y[idx]]\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dataset = TTPStyleDataset(X,Y)\n",
    "\n",
    "def my_collate_fn(data):\n",
    "    (xx,yy) = zip(*data)\n",
    "    x_lens = [len(x) for x in xx]\n",
    "    y_lens = [1 for y in yy]\n",
    "\n",
    "    xx_pad = pad_sequence(xx, batch_first=False, padding_value=0)\n",
    "    yy_pad = torch.tensor(yy)\n",
    "\n",
    "    return xx_pad, yy_pad, x_lens, y_lens\n",
    "    \n",
    "\n",
    "dataloader = DataLoader(input_dataset, shuffle=True, batch_size=16, num_workers=0, collate_fn=my_collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxRNN(nn.RNN):\n",
    "    def __init_subclass__(cls):\n",
    "        return super().__init_subclass__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_size - The number of expected features in the input x\n",
    "rnn = SoftmaxRNN(input_size=embed_dim, hidden_size=2, num_layers=1, nonlinearity='relu', bias=True, batch_first=False, dropout=0.0, bidirectional=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.813728  [   28/176400]\n",
      "loss: 0.558837  [ 2929/176400]\n",
      "loss: 0.843796  [ 5829/176400]\n",
      "loss: 0.581815  [ 8428/176400]\n",
      "loss: 0.571478  [12030/176400]\n",
      "loss: 0.445114  [14028/176400]\n",
      "loss: 0.666364  [17429/176400]\n",
      "loss: 0.658583  [18927/176400]\n",
      "loss: 0.495002  [24030/176400]\n",
      "loss: 0.555339  [25228/176400]\n",
      "loss: 0.711224  [28028/176400]\n",
      "loss: 0.492482  [33030/176400]\n",
      "loss: 0.558448  [33628/176400]\n",
      "loss: 0.443641  [39030/176400]\n",
      "loss: 0.738062  [40629/176400]\n",
      "loss: 0.547933  [43529/176400]\n",
      "loss: 0.615398  [44828/176400]\n",
      "loss: 0.722951  [52731/176400]\n",
      "loss: 0.696647  [48627/176400]\n",
      "loss: 0.612042  [55129/176400]\n",
      "loss: 0.380701  [56028/176400]\n",
      "loss: 0.675674  [60929/176400]\n",
      "loss: 0.662487  [63829/176400]\n",
      "loss: 0.656744  [69030/176400]\n",
      "loss: 0.708699  [69629/176400]\n",
      "loss: 0.623604  [70028/176400]\n",
      "loss: 0.653996  [75429/176400]\n",
      "loss: 0.397277  [78329/176400]\n",
      "loss: 0.729692  [81229/176400]\n",
      "loss: 0.456908  [84129/176400]\n",
      "loss: 0.678236  [87029/176400]\n",
      "loss: 0.506785  [86828/176400]\n",
      "loss: 0.580578  [89628/176400]\n",
      "loss: 0.655742  [99030/176400]\n",
      "loss: 0.530025  [95228/176400]\n",
      "loss: 0.624740  [101529/176400]\n",
      "loss: 0.676841  [100828/176400]\n",
      "loss: 0.567320  [107329/176400]\n",
      "loss: 0.562370  [106428/176400]\n",
      "loss: 0.620638  [117030/176400]\n",
      "loss: 0.568102  [116029/176400]\n",
      "loss: 0.464207  [118929/176400]\n",
      "loss: 0.499402  [117628/176400]\n",
      "loss: 0.433050  [124729/176400]\n",
      "loss: 0.674336  [132030/176400]\n",
      "loss: 0.770379  [130529/176400]\n",
      "loss: 0.616611  [138030/176400]\n",
      "loss: 0.624898  [131628/176400]\n",
      "loss: 0.489776  [139229/176400]\n",
      "loss: 0.715953  [137228/176400]\n",
      "loss: 0.407234  [140028/176400]\n",
      "loss: 0.570959  [147929/176400]\n",
      "loss: 0.646482  [145628/176400]\n",
      "loss: 0.618396  [153729/176400]\n",
      "loss: 0.527383  [156629/176400]\n",
      "loss: 0.790791  [159529/176400]\n",
      "loss: 0.520666  [162429/176400]\n",
      "loss: 0.566668  [148226/176400]\n",
      "loss: 0.599683  [162428/176400]\n",
      "loss: 0.613383  [171129/176400]\n",
      "loss: 0.672995  [174029/176400]\n",
      "loss: 0.599153  [183030/176400]\n",
      "loss: 0.714903  [173628/176400]\n",
      "loss: 0.511862  [176428/176400]\n",
      "loss: 0.373323  [179228/176400]\n",
      "loss: 0.729030  [175527/176400]\n",
      "loss: 0.671863  [184828/176400]\n",
      "loss: 0.511280  [194329/176400]\n",
      "loss: 0.511724  [190428/176400]\n",
      "loss: 0.479267  [186327/176400]\n",
      "loss: 0.399999  [182026/176400]\n",
      "loss: 0.654589  [205929/176400]\n",
      "loss: 0.726311  [201628/176400]\n",
      "loss: 0.621896  [211729/176400]\n",
      "loss: 0.461212  [214629/176400]\n",
      "loss: 0.662317  [210028/176400]\n",
      "loss: 0.510363  [205227/176400]\n",
      "loss: 0.728652  [215628/176400]\n",
      "loss: 0.661783  [234030/176400]\n",
      "loss: 0.449817  [237030/176400]\n",
      "loss: 0.763940  [224028/176400]\n",
      "loss: 0.468896  [226828/176400]\n",
      "loss: 0.622375  [237829/176400]\n",
      "loss: 0.775535  [240729/176400]\n",
      "loss: 0.623551  [235228/176400]\n",
      "loss: 0.770507  [246529/176400]\n",
      "loss: 0.674394  [232227/176400]\n",
      "loss: 0.501670  [252329/176400]\n",
      "loss: 0.621497  [237627/176400]\n",
      "loss: 0.763135  [258129/176400]\n",
      "loss: 0.663691  [243027/176400]\n",
      "loss: 0.557536  [263929/176400]\n",
      "loss: 0.650840  [276030/176400]\n",
      "loss: 0.749535  [269729/176400]\n",
      "loss: 0.624894  [272629/176400]\n",
      "loss: 0.654833  [275529/176400]\n",
      "loss: 0.663512  [259227/176400]\n",
      "loss: 0.566898  [300731/176400]\n",
      "loss: 0.461454  [284229/176400]\n",
      "loss: 0.568642  [277228/176400]\n",
      "loss: 0.440160  [280028/176400]\n",
      "loss: 0.724808  [292929/176400]\n",
      "loss: 0.577411  [285628/176400]\n",
      "loss: 0.738126  [298729/176400]\n",
      "loss: 0.555438  [301629/176400]\n",
      "loss: 0.557164  [283527/176400]\n",
      "loss: 0.595343  [318030/176400]\n",
      "loss: 0.560444  [310329/176400]\n",
      "loss: 0.660292  [302428/176400]\n",
      "loss: 0.571352  [305228/176400]\n",
      "loss: 0.776423  [319029/176400]\n",
      "loss: 0.820196  [   28/176400]\n",
      "loss: 0.722348  [ 2929/176400]\n",
      "loss: 0.363347  [ 6030/176400]\n",
      "loss: 0.722090  [ 8428/176400]\n",
      "loss: 0.471060  [11629/176400]\n",
      "loss: 0.797512  [14529/176400]\n",
      "loss: 0.704432  [16828/176400]\n",
      "loss: 0.568705  [20329/176400]\n",
      "loss: 0.557644  [23229/176400]\n",
      "loss: 0.616131  [25228/176400]\n",
      "loss: 0.605321  [29029/176400]\n",
      "loss: 0.712740  [31929/176400]\n",
      "loss: 0.768292  [36030/176400]\n",
      "loss: 0.628753  [35127/176400]\n",
      "loss: 0.611240  [39228/176400]\n",
      "loss: 0.642036  [42028/176400]\n",
      "loss: 0.513763  [44828/176400]\n",
      "loss: 0.725648  [45927/176400]\n",
      "loss: 0.674398  [52229/176400]\n",
      "loss: 0.619506  [55129/176400]\n",
      "loss: 0.567219  [58029/176400]\n",
      "loss: 0.522892  [58828/176400]\n",
      "loss: 0.499880  [66030/176400]\n",
      "loss: 0.567948  [69030/176400]\n",
      "loss: 0.610332  [67228/176400]\n",
      "loss: 0.673242  [75030/176400]\n",
      "loss: 0.502050  [72828/176400]\n",
      "loss: 0.514913  [78329/176400]\n",
      "loss: 0.563251  [81229/176400]\n",
      "loss: 0.627672  [87030/176400]\n",
      "loss: 0.722014  [90030/176400]\n",
      "loss: 0.558041  [86828/176400]\n",
      "loss: 0.553636  [96030/176400]\n",
      "loss: 0.748738  [99030/176400]\n",
      "loss: 0.557466  [98629/176400]\n",
      "loss: 0.446608  [98028/176400]\n",
      "loss: 0.493324  [104429/176400]\n",
      "loss: 0.334015  [103628/176400]\n",
      "loss: 0.670748  [102627/176400]\n",
      "loss: 0.671979  [113129/176400]\n",
      "loss: 0.716570  [116029/176400]\n",
      "loss: 0.671678  [110727/176400]\n",
      "loss: 0.613074  [117628/176400]\n",
      "loss: 0.461296  [120428/176400]\n",
      "loss: 0.519250  [127629/176400]\n",
      "loss: 0.562579  [135030/176400]\n",
      "loss: 0.568331  [128828/176400]\n",
      "loss: 0.668753  [136329/176400]\n",
      "loss: 0.823537  [139229/176400]\n",
      "loss: 0.671693  [147030/176400]\n",
      "loss: 0.520949  [150030/176400]\n",
      "loss: 0.338869  [153030/176400]\n",
      "loss: 0.509276  [150829/176400]\n",
      "loss: 0.626074  [153729/176400]\n",
      "loss: 0.560574  [156629/176400]\n",
      "loss: 0.653505  [170531/176400]\n",
      "loss: 0.671854  [156828/176400]\n",
      "loss: 0.454854  [171030/176400]\n",
      "loss: 0.676958  [162428/176400]\n",
      "loss: 0.512638  [171129/176400]\n",
      "loss: 0.616007  [168028/176400]\n",
      "loss: 0.658728  [176929/176400]\n",
      "loss: 0.495258  [173628/176400]\n",
      "loss: 0.671486  [182729/176400]\n",
      "loss: 0.615290  [185629/176400]\n",
      "loss: 0.468217  [188529/176400]\n",
      "loss: 0.615226  [191429/176400]\n",
      "loss: 0.626004  [194329/176400]\n",
      "loss: 0.511321  [197229/176400]\n",
      "loss: 0.784828  [200129/176400]\n",
      "loss: 0.513180  [203029/176400]\n",
      "loss: 0.569448  [213030/176400]\n",
      "loss: 0.676792  [208829/176400]\n",
      "loss: 0.665908  [211729/176400]\n",
      "loss: 0.567556  [207228/176400]\n",
      "loss: 0.770148  [210028/176400]\n",
      "loss: 0.621444  [212828/176400]\n",
      "loss: 0.621693  [215628/176400]\n",
      "loss: 0.469626  [226229/176400]\n",
      "loss: 0.600634  [221228/176400]\n",
      "loss: 0.654643  [224028/176400]\n",
      "loss: 0.622102  [243030/176400]\n",
      "loss: 0.560288  [229628/176400]\n",
      "loss: 0.673458  [240729/176400]\n",
      "loss: 0.714152  [243629/176400]\n",
      "loss: 0.659558  [255030/176400]\n",
      "loss: 0.563795  [249429/176400]\n",
      "loss: 0.616457  [243628/176400]\n",
      "loss: 0.568340  [246428/176400]\n",
      "loss: 0.660360  [258129/176400]\n",
      "loss: 0.807815  [252028/176400]\n",
      "loss: 0.722825  [263929/176400]\n",
      "loss: 0.609053  [266829/176400]\n",
      "loss: 0.768328  [279030/176400]\n",
      "loss: 0.711792  [272629/176400]\n",
      "loss: 0.622117  [256527/176400]\n",
      "loss: 0.565127  [278429/176400]\n",
      "loss: 0.544566  [281329/176400]\n",
      "loss: 0.664053  [284229/176400]\n",
      "loss: 0.559531  [277228/176400]\n",
      "loss: 0.825823  [280028/176400]\n",
      "loss: 0.714407  [303030/176400]\n",
      "loss: 0.571359  [316231/176400]\n",
      "loss: 0.521616  [288428/176400]\n",
      "loss: 0.490217  [312030/176400]\n",
      "loss: 0.487992  [294028/176400]\n",
      "loss: 0.516623  [296828/176400]\n",
      "loss: 0.515116  [310329/176400]\n",
      "loss: 0.572112  [302428/176400]\n",
      "loss: 0.513810  [316129/176400]\n",
      "loss: 0.455183  [319029/176400]\n",
      "loss: 0.512489  [   28/176400]\n",
      "loss: 0.616891  [ 2828/176400]\n",
      "loss: 0.613177  [ 5628/176400]\n",
      "loss: 0.861153  [ 8729/176400]\n",
      "loss: 0.624795  [12030/176400]\n",
      "loss: 0.566802  [14529/176400]\n",
      "loss: 0.776543  [18030/176400]\n",
      "loss: 0.669744  [20329/176400]\n",
      "loss: 0.555422  [23229/176400]\n",
      "loss: 0.666376  [25228/176400]\n",
      "loss: 0.669800  [29029/176400]\n",
      "loss: 0.503478  [30828/176400]\n",
      "loss: 0.677847  [33628/176400]\n",
      "loss: 0.583568  [33826/176400]\n",
      "loss: 0.565524  [40629/176400]\n",
      "loss: 0.862979  [42028/176400]\n",
      "loss: 0.599058  [48030/176400]\n",
      "loss: 0.608397  [49329/176400]\n",
      "loss: 0.628540  [48627/176400]\n",
      "loss: 0.553608  [57030/176400]\n",
      "loss: 0.556317  [56028/176400]\n",
      "loss: 0.537803  [65131/176400]\n",
      "loss: 0.658607  [63829/176400]\n",
      "loss: 0.804283  [64428/176400]\n",
      "loss: 0.623847  [69629/176400]\n",
      "loss: 0.668059  [67527/176400]\n",
      "loss: 0.664168  [72828/176400]\n",
      "loss: 0.579512  [78329/176400]\n",
      "loss: 0.779057  [75627/176400]\n",
      "loss: 0.505764  [78327/176400]\n",
      "loss: 0.663657  [87029/176400]\n",
      "loss: 0.659279  [93030/176400]\n",
      "loss: 0.610780  [89628/176400]\n",
      "loss: 0.597451  [95729/176400]\n",
      "loss: 0.448660  [98629/176400]\n",
      "loss: 0.535679  [98028/176400]\n",
      "loss: 0.435127  [97227/176400]\n",
      "loss: 0.499648  [103628/176400]\n",
      "loss: 0.679894  [110229/176400]\n",
      "loss: 0.467712  [109228/176400]\n",
      "loss: 0.546451  [116029/176400]\n",
      "loss: 0.509824  [114828/176400]\n",
      "loss: 0.570731  [126030/176400]\n",
      "loss: 0.505083  [116127/176400]\n",
      "loss: 0.561029  [132030/176400]\n",
      "loss: 0.619975  [135030/176400]\n",
      "loss: 0.655227  [128828/176400]\n",
      "loss: 0.555027  [126927/176400]\n",
      "loss: 0.441131  [134428/176400]\n",
      "loss: 0.453280  [147030/176400]\n",
      "loss: 0.502293  [150030/176400]\n",
      "loss: 0.471485  [142828/176400]\n",
      "loss: 0.622101  [145628/176400]\n",
      "loss: 0.666659  [153729/176400]\n",
      "loss: 0.601906  [145827/176400]\n",
      "loss: 0.758712  [159529/176400]\n",
      "loss: 0.617232  [156828/176400]\n",
      "loss: 0.513467  [165329/176400]\n",
      "loss: 0.642900  [162428/176400]\n",
      "loss: 0.666393  [165228/176400]\n",
      "loss: 0.735105  [174029/176400]\n",
      "loss: 0.684839  [176929/176400]\n",
      "loss: 0.464957  [179829/176400]\n",
      "loss: 0.543842  [176428/176400]\n",
      "loss: 0.567018  [185629/176400]\n",
      "loss: 0.564908  [182028/176400]\n",
      "loss: 0.628759  [198030/176400]\n",
      "loss: 0.729651  [201030/176400]\n",
      "loss: 0.564516  [197229/176400]\n",
      "loss: 0.656600  [200129/176400]\n",
      "loss: 0.460539  [203029/176400]\n",
      "loss: 0.622794  [213030/176400]\n",
      "loss: 0.665672  [216030/176400]\n",
      "loss: 0.573475  [219030/176400]\n",
      "loss: 0.613863  [207228/176400]\n",
      "loss: 0.625946  [217529/176400]\n",
      "loss: 0.610434  [220429/176400]\n",
      "loss: 0.513707  [223329/176400]\n",
      "loss: 0.416481  [226229/176400]\n",
      "loss: 0.729911  [221228/176400]\n",
      "loss: 0.743314  [224028/176400]\n",
      "loss: 0.567080  [234929/176400]\n",
      "loss: 0.574087  [246030/176400]\n",
      "loss: 0.613985  [232428/176400]\n",
      "loss: 0.567763  [252030/176400]\n",
      "loss: 0.621594  [238028/176400]\n",
      "loss: 0.670271  [258030/176400]\n",
      "loss: 0.712457  [243628/176400]\n",
      "loss: 0.571242  [255229/176400]\n",
      "loss: 0.622695  [258129/176400]\n",
      "loss: 0.617752  [261029/176400]\n",
      "loss: 0.463905  [263929/176400]\n",
      "loss: 0.696708  [266829/176400]\n",
      "loss: 0.691154  [269729/176400]\n",
      "loss: 0.616385  [272629/176400]\n",
      "loss: 0.382920  [266028/176400]\n",
      "loss: 0.494231  [268828/176400]\n",
      "loss: 0.512260  [271628/176400]\n",
      "loss: 0.628342  [264627/176400]\n",
      "loss: 0.609015  [287129/176400]\n",
      "loss: 0.614489  [280028/176400]\n",
      "loss: 0.600806  [282828/176400]\n",
      "loss: 0.487116  [285628/176400]\n",
      "loss: 0.528437  [298729/176400]\n",
      "loss: 0.660697  [312030/176400]\n",
      "loss: 0.540898  [294028/176400]\n",
      "loss: 0.520629  [318030/176400]\n",
      "loss: 0.564950  [299628/176400]\n",
      "loss: 0.611389  [324030/176400]\n",
      "loss: 0.620294  [327030/176400]\n",
      "loss: 0.566049  [330030/176400]\n",
      "loss: 0.601228  [   28/176400]\n",
      "loss: 0.608171  [ 2929/176400]\n",
      "loss: 0.623496  [ 5628/176400]\n",
      "loss: 0.597471  [ 8127/176400]\n",
      "loss: 0.652199  [11629/176400]\n",
      "loss: 0.681527  [14529/176400]\n",
      "loss: 0.495007  [18030/176400]\n",
      "loss: 0.560652  [20329/176400]\n",
      "loss: 0.665687  [23229/176400]\n",
      "loss: 0.683342  [25228/176400]\n",
      "loss: 0.741598  [29029/176400]\n",
      "loss: 0.676848  [31929/176400]\n",
      "loss: 0.618138  [33628/176400]\n",
      "loss: 0.455264  [39030/176400]\n",
      "loss: 0.786167  [42030/176400]\n",
      "loss: 0.608197  [42028/176400]\n",
      "loss: 0.559231  [48030/176400]\n",
      "loss: 0.626866  [49329/176400]\n",
      "loss: 0.626760  [52229/176400]\n",
      "loss: 0.746920  [55129/176400]\n",
      "loss: 0.727737  [56028/176400]\n",
      "loss: 0.716950  [60929/176400]\n",
      "loss: 0.682402  [66030/176400]\n",
      "loss: 0.786425  [66729/176400]\n",
      "loss: 0.618771  [69629/176400]\n",
      "loss: 0.612440  [67527/176400]\n",
      "loss: 0.482256  [70227/176400]\n",
      "loss: 0.604983  [72927/176400]\n",
      "loss: 0.497236  [81229/176400]\n",
      "loss: 0.521507  [84129/176400]\n",
      "loss: 0.677026  [87029/176400]\n",
      "loss: 0.579328  [89929/176400]\n",
      "loss: 0.660432  [92829/176400]\n",
      "loss: 0.616470  [95729/176400]\n",
      "loss: 0.670462  [95228/176400]\n",
      "loss: 0.716396  [98028/176400]\n",
      "loss: 0.578835  [108030/176400]\n",
      "loss: 0.622056  [103628/176400]\n",
      "loss: 0.727495  [110229/176400]\n",
      "loss: 0.523024  [117030/176400]\n",
      "loss: 0.556509  [116029/176400]\n",
      "loss: 0.444690  [110727/176400]\n",
      "loss: 0.556614  [117628/176400]\n",
      "loss: 0.567483  [124729/176400]\n",
      "loss: 0.513748  [132030/176400]\n",
      "loss: 0.509800  [135030/176400]\n",
      "loss: 0.674224  [133429/176400]\n",
      "loss: 0.515389  [136329/176400]\n",
      "loss: 0.797786  [139229/176400]\n",
      "loss: 0.652678  [132327/176400]\n",
      "loss: 0.610556  [145029/176400]\n",
      "loss: 0.566671  [147929/176400]\n",
      "loss: 0.570284  [150829/176400]\n",
      "loss: 0.612403  [143127/176400]\n",
      "loss: 0.708240  [156629/176400]\n",
      "loss: 0.557992  [148527/176400]\n",
      "loss: 0.783832  [168030/176400]\n",
      "loss: 0.646350  [159628/176400]\n",
      "loss: 0.680124  [156627/176400]\n",
      "loss: 0.692817  [171129/176400]\n",
      "loss: 0.621434  [174029/176400]\n",
      "loss: 0.513247  [170828/176400]\n",
      "loss: 0.570196  [173628/176400]\n",
      "loss: 0.567408  [176428/176400]\n",
      "loss: 0.597147  [185629/176400]\n",
      "loss: 0.728905  [201531/176400]\n",
      "loss: 0.547518  [191429/176400]\n",
      "loss: 0.651374  [201030/176400]\n",
      "loss: 0.557513  [190428/176400]\n",
      "loss: 0.554762  [193228/176400]\n",
      "loss: 0.559300  [203029/176400]\n",
      "loss: 0.519685  [205929/176400]\n",
      "loss: 0.513881  [194427/176400]\n",
      "loss: 0.680076  [211729/176400]\n",
      "loss: 0.719232  [207228/176400]\n",
      "loss: 0.832147  [202527/176400]\n",
      "loss: 0.497294  [220429/176400]\n",
      "loss: 0.387916  [200226/176400]\n",
      "loss: 0.667462  [210627/176400]\n",
      "loss: 0.449528  [221228/176400]\n",
      "loss: 0.745447  [224028/176400]\n",
      "loss: 0.595105  [218727/176400]\n",
      "loss: 0.625696  [229628/176400]\n",
      "loss: 0.514154  [232428/176400]\n",
      "loss: 0.766413  [235228/176400]\n",
      "loss: 0.566545  [229527/176400]\n",
      "loss: 0.474626  [258030/176400]\n",
      "loss: 0.520748  [252329/176400]\n",
      "loss: 0.502936  [255229/176400]\n",
      "loss: 0.570490  [267030/176400]\n",
      "loss: 0.520304  [252028/176400]\n",
      "loss: 0.559084  [263929/176400]\n",
      "loss: 0.571071  [276030/176400]\n",
      "loss: 0.603929  [269729/176400]\n",
      "loss: 0.645839  [272629/176400]\n",
      "loss: 0.702361  [266028/176400]\n",
      "loss: 0.491097  [268828/176400]\n",
      "loss: 0.490512  [281329/176400]\n",
      "loss: 0.646716  [303831/176400]\n",
      "loss: 0.663136  [277228/176400]\n",
      "loss: 0.610745  [290029/176400]\n",
      "loss: 0.567654  [282828/176400]\n",
      "loss: 0.678854  [295829/176400]\n",
      "loss: 0.547567  [298729/176400]\n",
      "loss: 0.615186  [312030/176400]\n",
      "loss: 0.654623  [294028/176400]\n",
      "loss: 0.792794  [296828/176400]\n",
      "loss: 0.499650  [299628/176400]\n",
      "loss: 0.502550  [324030/176400]\n",
      "loss: 0.660563  [316129/176400]\n",
      "loss: 0.732242  [330030/176400]\n",
      "loss: 0.509960  [   29/176400]\n",
      "loss: 0.568178  [ 2929/176400]\n",
      "loss: 0.547278  [ 6030/176400]\n",
      "loss: 0.743196  [ 8428/176400]\n",
      "loss: 0.570773  [11629/176400]\n",
      "loss: 0.668664  [15030/176400]\n",
      "loss: 0.696816  [17429/176400]\n",
      "loss: 0.558882  [19628/176400]\n",
      "loss: 0.506890  [24030/176400]\n",
      "loss: 0.525666  [25228/176400]\n",
      "loss: 0.723567  [30030/176400]\n",
      "loss: 0.705596  [31929/176400]\n",
      "loss: 0.831483  [36030/176400]\n",
      "loss: 0.610722  [39030/176400]\n",
      "loss: 0.567579  [42030/176400]\n",
      "loss: 0.569990  [45030/176400]\n",
      "loss: 0.673245  [48030/176400]\n",
      "loss: 0.666276  [49329/176400]\n",
      "loss: 0.568406  [54030/176400]\n",
      "loss: 0.774917  [55129/176400]\n",
      "loss: 0.725822  [52026/176400]\n",
      "loss: 0.491219  [60929/176400]\n",
      "loss: 0.567809  [61628/176400]\n",
      "loss: 0.717320  [69030/176400]\n",
      "loss: 0.502485  [64827/176400]\n",
      "loss: 0.555409  [72529/176400]\n",
      "loss: 0.655177  [72828/176400]\n",
      "loss: 0.775724  [81030/176400]\n",
      "loss: 0.511407  [81229/176400]\n",
      "loss: 0.508321  [84129/176400]\n",
      "loss: 0.577876  [90030/176400]\n",
      "loss: 0.661127  [89929/176400]\n",
      "loss: 0.559449  [89628/176400]\n",
      "loss: 0.753554  [95729/176400]\n",
      "loss: 0.679881  [95228/176400]\n",
      "loss: 0.551711  [98028/176400]\n",
      "loss: 0.512339  [108030/176400]\n",
      "loss: 0.522294  [107329/176400]\n",
      "loss: 0.700529  [110229/176400]\n",
      "loss: 0.620492  [113129/176400]\n",
      "loss: 0.343785  [112028/176400]\n",
      "loss: 0.681932  [123030/176400]\n",
      "loss: 0.720580  [117628/176400]\n",
      "loss: 0.380372  [124729/176400]\n",
      "loss: 0.678264  [127629/176400]\n",
      "loss: 0.505638  [126028/176400]\n",
      "loss: 0.774047  [133429/176400]\n",
      "loss: 0.675995  [141030/176400]\n",
      "loss: 0.519348  [139229/176400]\n",
      "loss: 0.657035  [137228/176400]\n",
      "loss: 0.790721  [145029/176400]\n",
      "loss: 0.613823  [137727/176400]\n",
      "loss: 0.677309  [145628/176400]\n",
      "loss: 0.623501  [143127/176400]\n",
      "loss: 0.643643  [151228/176400]\n",
      "loss: 0.516148  [159529/176400]\n",
      "loss: 0.467384  [168030/176400]\n",
      "loss: 0.667777  [159628/176400]\n",
      "loss: 0.571299  [174030/176400]\n",
      "loss: 0.623016  [171129/176400]\n",
      "loss: 0.509602  [168028/176400]\n",
      "loss: 0.567022  [164727/176400]\n",
      "loss: 0.558640  [179829/176400]\n",
      "loss: 0.729735  [176428/176400]\n",
      "loss: 0.525710  [192030/176400]\n",
      "loss: 0.705003  [188529/176400]\n",
      "loss: 0.617247  [191429/176400]\n",
      "loss: 0.610515  [194329/176400]\n",
      "loss: 0.725703  [197229/176400]\n",
      "loss: 0.502942  [200129/176400]\n",
      "loss: 0.716880  [189027/176400]\n",
      "loss: 0.765122  [198828/176400]\n",
      "loss: 0.630347  [216030/176400]\n",
      "loss: 0.490767  [211729/176400]\n",
      "loss: 0.613772  [214629/176400]\n",
      "loss: 0.603976  [217529/176400]\n",
      "loss: 0.571695  [212828/176400]\n",
      "loss: 0.648460  [215628/176400]\n",
      "loss: 0.671324  [218428/176400]\n",
      "loss: 0.621674  [221228/176400]\n",
      "loss: 0.621038  [232029/176400]\n",
      "loss: 0.629763  [226828/176400]\n",
      "loss: 0.610132  [237829/176400]\n",
      "loss: 0.570763  [224127/176400]\n",
      "loss: 0.715834  [243629/176400]\n",
      "loss: 0.658959  [246529/176400]\n",
      "loss: 0.720356  [249429/176400]\n",
      "loss: 0.727481  [261030/176400]\n",
      "loss: 0.690057  [255229/176400]\n",
      "loss: 0.549931  [240327/176400]\n",
      "loss: 0.455056  [252028/176400]\n",
      "loss: 0.594882  [263929/176400]\n",
      "loss: 0.466732  [266829/176400]\n",
      "loss: 0.701969  [269729/176400]\n",
      "loss: 0.621558  [263228/176400]\n",
      "loss: 0.514294  [275529/176400]\n",
      "loss: 0.623467  [288030/176400]\n",
      "loss: 0.609147  [261927/176400]\n",
      "loss: 0.499299  [284229/176400]\n",
      "loss: 0.622317  [287129/176400]\n",
      "loss: 0.695435  [290029/176400]\n",
      "loss: 0.697118  [262626/176400]\n",
      "loss: 0.567576  [316231/176400]\n",
      "loss: 0.516725  [288428/176400]\n",
      "loss: 0.609610  [322431/176400]\n",
      "loss: 0.419054  [304529/176400]\n",
      "loss: 0.441391  [307429/176400]\n",
      "loss: 0.624463  [310329/176400]\n",
      "loss: 0.563202  [291627/176400]\n",
      "loss: 0.718050  [305228/176400]\n",
      "loss: 0.620518  [308028/176400]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn\n",
    "optimizer = torch.optim.SGD(rnn.parameters(), lr=learning_rate)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "size = len(dataloader.dataset)\n",
    "for epoch in range(num_epochs):\n",
    "    for batch,(x_padded, y_padded, x_lens, y_lens) in enumerate(dataloader):\n",
    "        \n",
    "        rnn.zero_grad()\n",
    "        x_embed = embed(x_padded)\n",
    "        x_packed = pack_padded_sequence(x_embed, x_lens, batch_first=False, enforce_sorted=False)\n",
    "        output_packed, hidden = rnn(x_packed)\n",
    "        output, seq_len = pad_packed_sequence(output_packed, batch_first=False)\n",
    "        hidden = torch.reshape(hidden, (16,2))\n",
    "        y_padded = torch.reshape(y_padded, (16,))\n",
    "        y_padded = y_padded.long()\n",
    "        loss = criterion(hidden, y_padded)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        if batch % 10000 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(x_padded)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# Example of target with class indices\n",
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "output = loss(input, target)\n",
    "\n",
    "print(np.shape(input))\n",
    "print(np.shape(target))\n",
    "output.backward()\n",
    "# Example of target with class probabilities\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5).softmax(dim=1)\n",
    "output = loss(input, target)\n",
    "output.backward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
